Running on the GPU0

#### starting new seed ####

random seed is: 11135967900375438520
model's performance on test data:
mse: 37.45471084529431, rmse: 6.120025395804686, mean: 0.7243475006679072
std: 6.077312312328884, min: -10.850701972124668, max: 12.303193686718567

iteration: 0, loss: 37.147375, current train MSE: 37.29788113054611, lowers train MSE: 37.29788113054611, training time: 0.6672s
model's performance on test data:
mse: 0.12671417971959573, rmse: 0.3559693522195355, mean: -0.007824295316150419
std: 0.3559011474568284, min: -0.6158739474124202, max: 0.5570815707769157

iteration: 2000, loss: -9.941498, current train MSE: 0.17994217628086961, lowers train MSE: 0.1418457346384858, training time: 85.5294s
model's performance on test data:
mse: 0.14011117980249288, rmse: 0.37431427945309925, mean: 0.08647776740935358
std: 0.36420604090147984, min: -0.6062426579795019, max: 0.7106667672326878

iteration: 4000, loss: -10.543796, current train MSE: 0.2960142350114721, lowers train MSE: 0.1393529343073067, training time: 174.7096s
Formula: 0.966*x_2 + 0.029*sin(x_1) + 0.524
model's performance on test data:
mse: 41.25667258708951, rmse: 6.423135728527734, mean: 1.5770958436353297
std: 6.22682251575457, min: -7.073751490514693, max: 16.362904239750087

iteration: 0, loss: 42.503937, current train MSE: 41.86671570113371, lowers train MSE: 41.86671570113371, training time: 0.0830s
model's performance on test data:
mse: 0.13325654103407364, rmse: 0.3650432043389846, mean: 0.03632357864929192
std: 0.36324968517030554, min: -0.6313209876152834, max: 0.6905411544372217

iteration: 2000, loss: -9.850033, current train MSE: 0.3085009624016992, lowers train MSE: 0.13263531724150102, training time: 84.0174s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13325654103407364, rmse: 0.3650432043389846, mean: 0.03632357864929192
std: 0.36324968517030554, min: -0.6313209876152834, max: 0.6905411544372217

iteration: 4000, loss: -10.288971, current train MSE: 0.17355096646403315, lowers train MSE: 0.13263531724150102, training time: 175.0132s

##########
reduced lr to 1e-05
##########

Formula: 0.998*x_2 + 0.012*sin(x_1) + 0.466
model's performance on test data:
mse: 34.31244771377858, rmse: 5.857682793885188, mean: 1.4456076962690518
std: 5.676785068256721, min: -9.87555698426058, max: 14.496914845099186

iteration: 0, loss: 41.128237, current train MSE: 37.68018105591253, lowers train MSE: 37.68018105591253, training time: 0.0789s
model's performance on test data:
mse: 0.14306317132240165, rmse: 0.3782369248531952, mean: 0.05999134467572674
std: 0.373467746646519, min: -0.9197233520870469, max: 0.7520374590200354

iteration: 2000, loss: -9.115646, current train MSE: 0.5693293496559109, lowers train MSE: 0.1412950531427966, training time: 83.2037s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14306317132240165, rmse: 0.3782369248531952, mean: 0.05999134467572674
std: 0.373467746646519, min: -0.9197233520870469, max: 0.7520374590200354

iteration: 4000, loss: -9.756657, current train MSE: 0.21521745433693165, lowers train MSE: 0.1412950531427966, training time: 172.2734s

##########
reduced lr to 1e-05
##########

Formula: 0.996*x_2 + 0.386
model's performance on test data:
mse: 39.0087420595312, rmse: 6.245697884106403, mean: 1.284963396175988
std: 6.1123929262025944, min: -9.645276392235166, max: 13.634135991994222

iteration: 0, loss: 29.251087, current train MSE: 28.73530306669533, lowers train MSE: 28.73530306669533, training time: 0.1190s
model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 2000, loss: -9.650042, current train MSE: 0.24050046197052202, lowers train MSE: 0.13366474922240146, training time: 82.8501s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 4000, loss: -10.643520, current train MSE: 0.17150098097140637, lowers train MSE: 0.13366474922240146, training time: 171.8618s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 6000, loss: -10.720229, current train MSE: 0.16819637225642348, lowers train MSE: 0.13366474922240146, training time: 269.6781s
model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 8000, loss: -10.731820, current train MSE: 0.16729531010428336, lowers train MSE: 0.13366474922240146, training time: 375.3904s
Formula: 1.0*x_2 + 0.49
model's performance on test data:
mse: 34.86338757624688, rmse: 5.904522637457399, mean: 0.32439150873339767
std: 5.895899753992503, min: -14.417256900992513, max: 15.263414149119988

iteration: 0, loss: 25.687250, current train MSE: 25.298984931432052, lowers train MSE: 25.298984931432052, training time: 0.0865s
model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 2000, loss: -9.437240, current train MSE: 0.27854044557154045, lowers train MSE: 0.12506585134406256, training time: 83.9418s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 4000, loss: -10.468262, current train MSE: 0.17087610234919373, lowers train MSE: 0.12506585134406256, training time: 175.1783s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 6000, loss: -10.551176, current train MSE: 0.16723454465446325, lowers train MSE: 0.12506585134406256, training time: 273.5978s
model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 8000, loss: -10.567824, current train MSE: 0.16332044896982545, lowers train MSE: 0.12506585134406256, training time: 381.1570s
Formula: 0.995*x_2 + 0.489
model's performance on test data:
mse: 34.90426143044104, rmse: 5.907982856308999, mean: 1.5442564417466467
std: 5.702875219666036, min: -8.138267669211764, max: 15.783050697109527

iteration: 0, loss: 41.334886, current train MSE: 37.4833301375671, lowers train MSE: 37.4833301375671, training time: 0.7217s
model's performance on test data:
mse: 0.13453963080562198, rmse: 0.36679644328376737, mean: 0.026854961525629835
std: 0.36583032285055256, min: -0.643073346043165, max: 0.6701227177925198

iteration: 2000, loss: -9.162098, current train MSE: 0.21245885797461353, lowers train MSE: 0.12319483157105215, training time: 83.8579s
model's performance on test data:
mse: 0.13330627680732188, rmse: 0.36511132111634376, mean: 0.0013791123083190934
std: 0.3651269734032412, min: -0.6618894367242039, max: 0.616055867658055

iteration: 4000, loss: -10.252181, current train MSE: 0.32893059045759465, lowers train MSE: 0.11681657563115486, training time: 175.3435s
model's performance on test data:
mse: 0.13730787362495814, rmse: 0.37055077064412933, mean: 0.006598446418123658
std: 0.3705105423829134, min: -0.6933293283353752, max: 0.6567989721380627

iteration: 6000, loss: -10.752544, current train MSE: 0.3540717174885806, lowers train MSE: 0.11373369234683839, training time: 273.5464s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13730787362495814, rmse: 0.37055077064412933, mean: 0.006598446418123658
std: 0.3705105423829134, min: -0.6933293283353752, max: 0.6567989721380627

iteration: 8000, loss: -11.044819, current train MSE: 0.1800848077230915, lowers train MSE: 0.11373369234683839, training time: 378.8259s

##########
reduced lr to 1e-05
##########

Formula: 1.002*x_2 + 0.502
model's performance on test data:
mse: 37.699330761268705, rmse: 6.139978074982736, mean: 0.752240466110674
std: 6.094028124828144, min: -12.375359684889911, max: 14.600118178065403

iteration: 0, loss: 39.592081, current train MSE: 39.80998078195141, lowers train MSE: 39.80998078195141, training time: 0.1161s
model's performance on test data:
mse: 0.13205231851267582, rmse: 0.3633900363420492, mean: 0.001278535352270356
std: 0.3634059580317572, min: -0.8615818113239637, max: 0.6414647474726451

iteration: 2000, loss: -9.825775, current train MSE: 0.25742647929852625, lowers train MSE: 0.13219745324494991, training time: 83.3227s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 4000, loss: -10.457953, current train MSE: 0.3096562950723003, lowers train MSE: 0.12691348942004713, training time: 172.8403s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 6000, loss: -10.846804, current train MSE: 0.16462830696823455, lowers train MSE: 0.12691348942004713, training time: 271.1688s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 8000, loss: -10.877090, current train MSE: 0.1643186331506001, lowers train MSE: 0.12691348942004713, training time: 376.9691s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 10000, loss: -10.885309, current train MSE: 0.1629496901208149, lowers train MSE: 0.12691348942004713, training time: 489.9199s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 12000, loss: -10.890953, current train MSE: 0.16475711455730802, lowers train MSE: 0.12691348942004713, training time: 612.6115s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 14000, loss: -10.897660, current train MSE: 0.16587431715112425, lowers train MSE: 0.12691348942004713, training time: 741.6621s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 16000, loss: -10.908551, current train MSE: 0.16259185127137335, lowers train MSE: 0.12691348942004713, training time: 879.9199s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 18000, loss: -10.913285, current train MSE: 0.16523610643411485, lowers train MSE: 0.12691348942004713, training time: 1025.1666s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 20000, loss: -10.920365, current train MSE: 0.16530193030205798, lowers train MSE: 0.12691348942004713, training time: 1180.7317s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 22000, loss: -10.927333, current train MSE: 0.16526754082923967, lowers train MSE: 0.12691348942004713, training time: 1341.4563s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 24000, loss: -10.934111, current train MSE: 0.1652446290140634, lowers train MSE: 0.12691348942004713, training time: 1510.9337s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 26000, loss: -10.940844, current train MSE: 0.16508793802892274, lowers train MSE: 0.12691348942004713, training time: 1690.6282s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 28000, loss: -10.947275, current train MSE: 0.16506482648033444, lowers train MSE: 0.12691348942004713, training time: 1878.5800s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 29250, loss: -10.951495, current train MSE: 0.16476654485188102, lowers train MSE: 0.12691348942004713, training time: 2000.1296s
Formula: 1.006*x_2 + 0.488
model's performance on test data:
mse: 26.177243346449444, rmse: 5.116370133839951, mean: 0.3996858638204521
std: 5.100989764720858, min: -9.280107345171448, max: 10.125835585409915

iteration: 0, loss: 23.583269, current train MSE: 23.624221876423196, lowers train MSE: 23.624221876423196, training time: 0.1117s
model's performance on test data:
mse: 0.13133947474746913, rmse: 0.3624078844995913, mean: 0.03633270792150655
std: 0.36060007267018207, min: -0.724822827183349, max: 0.6277924032530242

iteration: 2000, loss: -9.943806, current train MSE: 0.26236146286287954, lowers train MSE: 0.12876434630136674, training time: 84.9562s
model's performance on test data:
mse: 0.13216815335330417, rmse: 0.36354938227605915, mean: 0.02445699357879713
std: 0.36274394167453117, min: -0.7555586649508736, max: 0.626571355421393

iteration: 4000, loss: -10.666364, current train MSE: 0.1907389872044917, lowers train MSE: 0.12429969575316341, training time: 174.6334s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 6000, loss: -10.872722, current train MSE: 0.3087170500374795, lowers train MSE: 0.11982482445382381, training time: 272.1031s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 8000, loss: -11.094021, current train MSE: 0.19016639388173734, lowers train MSE: 0.11982482445382381, training time: 379.2597s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 10000, loss: -11.106212, current train MSE: 0.19104853762339835, lowers train MSE: 0.11982482445382381, training time: 492.6758s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 12000, loss: -11.112123, current train MSE: 0.18866112258609447, lowers train MSE: 0.11982482445382381, training time: 613.9994s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 14000, loss: -11.116055, current train MSE: 0.18851374929143408, lowers train MSE: 0.11982482445382381, training time: 744.9087s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 16000, loss: -11.119833, current train MSE: 0.18841080676192817, lowers train MSE: 0.11982482445382381, training time: 883.7212s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 18000, loss: -11.124132, current train MSE: 0.18767925686109402, lowers train MSE: 0.11982482445382381, training time: 1030.7056s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 20000, loss: -11.126282, current train MSE: 0.1889717755661139, lowers train MSE: 0.11982482445382381, training time: 1184.8451s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 22000, loss: -11.131449, current train MSE: 0.18715408447479392, lowers train MSE: 0.11982482445382381, training time: 1346.8005s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 24000, loss: -11.134406, current train MSE: 0.1874665799182336, lowers train MSE: 0.11982482445382381, training time: 1520.0757s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 26000, loss: -11.137451, current train MSE: 0.18761556945251692, lowers train MSE: 0.11982482445382381, training time: 1703.2747s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 28000, loss: -11.141108, current train MSE: 0.18708805263411113, lowers train MSE: 0.11982482445382381, training time: 1893.8265s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 29060, loss: -11.142420, current train MSE: 0.18740675859688782, lowers train MSE: 0.11982482445382381, training time: 2000.2671s
Formula: 1.001*x_2 + 0.495
model's performance on test data:
mse: 31.433775601213426, rmse: 5.60658323769597, mean: -0.10841820170966648
std: 5.605815162307112, min: -11.131717570435717, max: 11.062330770376128

iteration: 0, loss: 33.436509, current train MSE: 29.65310470010033, lowers train MSE: 29.65310470010033, training time: 0.1126s
model's performance on test data:
mse: 0.1510690207715723, rmse: 0.388675984299998, mean: 0.07567255004552738
std: 0.38125742188920364, min: -0.9571691123450456, max: 0.8138475263353893

iteration: 2000, loss: -8.470937, current train MSE: 0.5847871174040729, lowers train MSE: 0.1995571519256292, training time: 81.6304s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 4000, loss: -9.867337, current train MSE: 0.4413355584594274, lowers train MSE: 0.12462668623188607, training time: 170.4937s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 6000, loss: -10.717756, current train MSE: 0.16389733206272877, lowers train MSE: 0.12462668623188607, training time: 266.6277s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 8000, loss: -10.772285, current train MSE: 0.16351437876936725, lowers train MSE: 0.12462668623188607, training time: 371.3101s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 10000, loss: -10.779462, current train MSE: 0.1647086005304877, lowers train MSE: 0.12462668623188607, training time: 483.6364s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 12000, loss: -10.785591, current train MSE: 0.1680251310637463, lowers train MSE: 0.12462668623188607, training time: 607.0226s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 14000, loss: -10.800190, current train MSE: 0.16353198856018353, lowers train MSE: 0.12462668623188607, training time: 737.9143s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 16000, loss: -10.812976, current train MSE: 0.1604934916428257, lowers train MSE: 0.12462668623188607, training time: 875.2501s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 18000, loss: -10.821834, current train MSE: 0.16115108456761928, lowers train MSE: 0.12462668623188607, training time: 1020.0927s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 20000, loss: -10.832280, current train MSE: 0.16010803798792153, lowers train MSE: 0.12462668623188607, training time: 1172.7969s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 22000, loss: -10.837283, current train MSE: 0.16412041102471286, lowers train MSE: 0.12462668623188607, training time: 1332.7853s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 24000, loss: -10.846338, current train MSE: 0.16390943549181158, lowers train MSE: 0.12462668623188607, training time: 1503.7574s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 26000, loss: -10.857309, current train MSE: 0.1615700705319778, lowers train MSE: 0.12462668623188607, training time: 1684.9957s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 28000, loss: -10.865695, current train MSE: 0.16162803041792412, lowers train MSE: 0.12462668623188607, training time: 1874.7438s
2000 seconds time exceeded

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 29280, loss: -10.869115, current train MSE: 0.16352981980478232, lowers train MSE: 0.12462668623188607, training time: 2000.7182s
Formula: 0.997*x_2 + 0.497*cos(0.059*x_2)
model's performance on test data:
mse: 31.28082757860165, rmse: 5.59292656653041, mean: 0.29357541363177697
std: 5.585495577577269, min: -10.778585389786628, max: 10.47168909776954

iteration: 0, loss: 27.604341, current train MSE: 27.50728528719557, lowers train MSE: 27.50728528719557, training time: 0.0857s
model's performance on test data:
mse: 0.1403487651943619, rmse: 0.3746315058752559, mean: 0.04514759456488196
std: 0.37191973919261634, min: -0.9189460001305942, max: 0.7677000506670821

iteration: 2000, loss: -9.456577, current train MSE: 0.43944413033223917, lowers train MSE: 0.16714082781571152, training time: 82.0137s
model's performance on test data:
mse: 0.13564775295302856, rmse: 0.3683038866928077, mean: -0.007858964063253355
std: 0.368238441284421, min: -0.7924723214874128, max: 0.634540352503679

iteration: 4000, loss: -10.390025, current train MSE: 0.3897952917733898, lowers train MSE: 0.10723286026536519, training time: 170.0293s
Formula: 1.06*x_2 + 0.451
model's performance on test data:
mse: 33.81027581867358, rmse: 5.814660421613079, mean: 0.3558871901500577
std: 5.804049349214654, min: -12.284846055273729, max: 14.258755784749308

iteration: 0, loss: 32.678529, current train MSE: 32.74746727757576, lowers train MSE: 32.74746727757576, training time: 0.1116s
model's performance on test data:
mse: 0.1311897500877052, rmse: 0.3622012563309316, mean: 0.02477856165213499
std: 0.36137076794608136, min: -0.6627261493265326, max: 0.6812516618664155

iteration: 2000, loss: -10.013976, current train MSE: 0.16873396185074063, lowers train MSE: 0.1309767918849672, training time: 84.2643s
model's performance on test data:
mse: 0.13432040695625216, rmse: 0.36649748560699863, mean: 0.03346753124329169
std: 0.36498445548815334, min: -0.6729278650870452, max: 0.6940085193106018

iteration: 4000, loss: -10.664859, current train MSE: 0.15673714734839322, lowers train MSE: 0.1282540353378009, training time: 174.6174s
Formula: 0.963*x_2 - 0.039*cos(x_1) + 0.409
model's performance on test data:
mse: 43.58491829659982, rmse: 6.601887479849972, mean: 0.286783948995869
std: 6.5959854452966855, min: -13.38784586710791, max: 12.258763756067635

iteration: 0, loss: 70.646412, current train MSE: 66.79271983275915, lowers train MSE: 66.79271983275915, training time: 0.4371s
model's performance on test data:
mse: 0.1504246310603237, rmse: 0.3878461435418995, mean: 0.05763234805797426
std: 0.38355945485482523, min: -0.8296244636598455, max: 0.7966713380026125

iteration: 2000, loss: -8.643030, current train MSE: 0.17727067994901904, lowers train MSE: 0.124644519188442, training time: 83.6615s
model's performance on test data:
mse: 0.13814381472294926, rmse: 0.37167703012555037, mean: -0.027238314846614074
std: 0.3706961432036334, min: -0.7373474786478962, max: 0.672059029878989

iteration: 4000, loss: -10.061888, current train MSE: 0.37863252188234925, lowers train MSE: 0.12091969718584239, training time: 173.7664s

##########
reduced lr to 0.0001
##########

Formula: 1.008*x_2 + 0.495
model's performance on test data:
mse: 39.31414653500765, rmse: 6.270099403917585, mean: 1.673138732417123
std: 6.043046018161045, min: -11.17016306671892, max: 18.61646942178563

iteration: 0, loss: 49.001638, current train MSE: 49.140778237645044, lowers train MSE: 49.140778237645044, training time: 0.4388s
model's performance on test data:
mse: 0.13130333562843402, rmse: 0.36235802133861206, mean: 0.053480738769945406
std: 0.3584075778883216, min: -0.7138144818788117, max: 0.6730542103721682

iteration: 2000, loss: -9.291553, current train MSE: 0.17699219764506213, lowers train MSE: 0.12325100318604139, training time: 84.6212s
model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 4000, loss: -10.114221, current train MSE: 0.5083400320178373, lowers train MSE: 0.10598698884904828, training time: 175.0326s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 6000, loss: -10.895455, current train MSE: 0.2007119651764338, lowers train MSE: 0.10598698884904828, training time: 273.0951s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 8000, loss: -10.961466, current train MSE: 0.1773583981716092, lowers train MSE: 0.10598698884904828, training time: 379.3784s
Formula: -0.004*x_1 + 0.999*x_2 + 0.502
model's performance on test data:
mse: 33.43556321911492, rmse: 5.782349282005967, mean: 0.919785905865466
std: 5.709011859140262, min: -8.873012605263837, max: 11.999940055058907

iteration: 0, loss: 27.727467, current train MSE: 27.26097362396836, lowers train MSE: 27.26097362396836, training time: 0.0886s
model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 2000, loss: -9.852144, current train MSE: 0.1595109437239794, lowers train MSE: 0.12238034580480973, training time: 83.2260s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 4000, loss: -10.223362, current train MSE: 0.15915216913088343, lowers train MSE: 0.12238034580480973, training time: 175.7477s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 6000, loss: -10.259859, current train MSE: 0.16148187657360175, lowers train MSE: 0.12238034580480973, training time: 276.3850s
model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 8000, loss: -10.280509, current train MSE: 0.1625284450583011, lowers train MSE: 0.12238034580480973, training time: 386.5689s
Formula: 0.992*x_2 + 0.497
model's performance on test data:
mse: 33.897728989359884, rmse: 5.8221756233696595, mean: 0.965553300046171
std: 5.741840533048874, min: -10.275966716691537, max: 12.898155869068502

iteration: 0, loss: 35.176470, current train MSE: 31.847720090609897, lowers train MSE: 31.847720090609897, training time: 0.7265s
model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 2000, loss: -9.168069, current train MSE: 0.39372344954809746, lowers train MSE: 0.11067269318254061, training time: 86.6898s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 4000, loss: -10.511758, current train MSE: 0.16904086560859066, lowers train MSE: 0.11067269318254061, training time: 178.5908s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 6000, loss: -10.598508, current train MSE: 0.17086150907282982, lowers train MSE: 0.11067269318254061, training time: 277.5071s
model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 8000, loss: -10.612454, current train MSE: 0.16871257790388539, lowers train MSE: 0.11067269318254061, training time: 386.2504s
Formula: 1.004*x_2 + 0.5
model's performance on test data:
mse: 30.06686197502257, rmse: 5.483325813320103, mean: -0.03975748888116929
std: 5.483455859810432, min: -11.504065782554813, max: 9.505307494121128

iteration: 0, loss: 25.169219, current train MSE: 25.41213775465784, lowers train MSE: 25.41213775465784, training time: 0.1112s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 2000, loss: -9.173949, current train MSE: 0.28338096399422824, lowers train MSE: 0.11264185686582269, training time: 87.9192s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 4000, loss: -10.277914, current train MSE: 0.17477391839556514, lowers train MSE: 0.11264185686582269, training time: 180.7620s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 6000, loss: -10.385690, current train MSE: 0.16198268178976663, lowers train MSE: 0.11264185686582269, training time: 280.4097s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 8000, loss: -10.407247, current train MSE: 0.1556099371493551, lowers train MSE: 0.11264185686582269, training time: 388.8095s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 10000, loss: -10.420168, current train MSE: 0.15959530079555342, lowers train MSE: 0.11264185686582269, training time: 503.5911s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 12000, loss: -10.439760, current train MSE: 0.1570754537774507, lowers train MSE: 0.11264185686582269, training time: 626.3560s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 14000, loss: -10.455421, current train MSE: 0.15807604287168514, lowers train MSE: 0.11264185686582269, training time: 755.5141s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 16000, loss: -10.471918, current train MSE: 0.15749002442304982, lowers train MSE: 0.11264185686582269, training time: 893.0322s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 18000, loss: -10.485260, current train MSE: 0.15952118831849682, lowers train MSE: 0.11264185686582269, training time: 1037.5935s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 20000, loss: -10.499917, current train MSE: 0.15954475442056198, lowers train MSE: 0.11264185686582269, training time: 1190.1754s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 22000, loss: -10.515648, current train MSE: 0.15831362780785108, lowers train MSE: 0.11264185686582269, training time: 1350.4016s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 24000, loss: -10.530554, current train MSE: 0.15742955288656257, lowers train MSE: 0.11264185686582269, training time: 1519.4886s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 26000, loss: -10.542786, current train MSE: 0.15895756304171665, lowers train MSE: 0.11264185686582269, training time: 1700.3789s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 28000, loss: -10.553310, current train MSE: 0.161882149564666, lowers train MSE: 0.11264185686582269, training time: 1890.5398s
2000 seconds time exceeded

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 29070, loss: -10.545724, current train MSE: 0.17650431516911694, lowers train MSE: 0.11264185686582269, training time: 2000.3079s
Formula: -0.007*x_1 + 0.993*x_2 + 0.063*sin(0.044*x_1 + 0.044*x_2) + 0.448*cos(0.044*x_1 + 0.044*x_2) + 0.048
model's performance on test data:
mse: 38.38875366979218, rmse: 6.195865853114654, mean: 1.524688708816288
std: 6.005637751812585, min: -10.382761669700095, max: 14.031421058144113

iteration: 0, loss: 36.962234, current train MSE: 36.79381789347789, lowers train MSE: 36.79381789347789, training time: 1.4439s
model's performance on test data:
mse: 0.12870386821661672, rmse: 0.35875321352793027, mean: -0.009932561262568319
std: 0.3586336213271653, min: -0.6343853872734124, max: 0.6185774542741029

iteration: 2000, loss: -9.870931, current train MSE: 0.1677073737049436, lowers train MSE: 0.12551641524020798, training time: 84.4016s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 4000, loss: -10.487863, current train MSE: 0.3074774948184264, lowers train MSE: 0.1163184025182096, training time: 172.8625s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 6000, loss: -10.936473, current train MSE: 0.19184686509313634, lowers train MSE: 0.1163184025182096, training time: 270.6708s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 8000, loss: -10.980923, current train MSE: 0.1796865418740439, lowers train MSE: 0.1163184025182096, training time: 376.6950s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 10000, loss: -10.987395, current train MSE: 0.1780667942731924, lowers train MSE: 0.1163184025182096, training time: 489.4560s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 12000, loss: -10.992901, current train MSE: 0.1781528553757266, lowers train MSE: 0.1163184025182096, training time: 609.3949s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 14000, loss: -10.999009, current train MSE: 0.17783472231907962, lowers train MSE: 0.1163184025182096, training time: 737.8037s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 16000, loss: -11.004507, current train MSE: 0.1780729374256778, lowers train MSE: 0.1163184025182096, training time: 873.5445s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 18000, loss: -11.010143, current train MSE: 0.1780185888115269, lowers train MSE: 0.1163184025182096, training time: 1015.7487s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 20000, loss: -11.015752, current train MSE: 0.17781170184986156, lowers train MSE: 0.1163184025182096, training time: 1166.6599s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 22000, loss: -11.020652, current train MSE: 0.1781428988374986, lowers train MSE: 0.1163184025182096, training time: 1324.6964s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 24000, loss: -11.025704, current train MSE: 0.17816857972975103, lowers train MSE: 0.1163184025182096, training time: 1492.0871s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 26000, loss: -11.030815, current train MSE: 0.1779984366961111, lowers train MSE: 0.1163184025182096, training time: 1669.9829s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 28000, loss: -11.035515, current train MSE: 0.17810390303432813, lowers train MSE: 0.1163184025182096, training time: 1856.3358s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 29480, loss: -11.044248, current train MSE: 0.17287248646271425, lowers train MSE: 0.1163184025182096, training time: 2000.9228s
Formula: 1.002*x_2 + 0.498
model's performance on test data:
mse: 51.62797057078056, rmse: 7.185260647379506, mean: -2.2761354450085443
std: 6.81555743966151, min: -20.336238163279482, max: 5.381503464370757

iteration: 0, loss: 34.792433, current train MSE: 31.501043969368993, lowers train MSE: 31.501043969368993, training time: 0.1311s
model's performance on test data:
mse: 0.1332012517787885, rmse: 0.36496746674024044, mean: 0.07977495143457808
std: 0.3561599275006231, min: -0.5450062823483481, max: 0.6270494762474037

iteration: 2000, loss: -9.702247, current train MSE: 0.29805097450211254, lowers train MSE: 0.12877807262012847, training time: 81.7063s
model's performance on test data:
mse: 0.13628162329481672, rmse: 0.36916341001623754, mean: 0.007192684019895701
std: 0.36911178932360356, min: -0.6995627448992305, max: 0.646647965812635

iteration: 4000, loss: -10.585455, current train MSE: 0.22928770822052985, lowers train MSE: 0.12374682708727891, training time: 169.0355s
model's performance on test data:
mse: 0.12917123108116887, rmse: 0.3594039942476556, mean: 0.03347079696952259
std: 0.3578599492421924, min: -0.5905309464042574, max: 0.6057329993781089

iteration: 6000, loss: -10.882002, current train MSE: 0.27639260903102214, lowers train MSE: 0.12203019090432439, training time: 263.5159s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 8000, loss: -11.051602, current train MSE: 0.27691050900030323, lowers train MSE: 0.11933059413226069, training time: 367.3108s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 10000, loss: -11.201210, current train MSE: 0.14711622714675068, lowers train MSE: 0.11933059413226069, training time: 477.4799s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 12000, loss: -11.200007, current train MSE: 0.15135773625936325, lowers train MSE: 0.11933059413226069, training time: 594.3353s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 14000, loss: -11.203195, current train MSE: 0.15060843151514575, lowers train MSE: 0.11933059413226069, training time: 722.7645s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 16000, loss: -11.206699, current train MSE: 0.1495173378314704, lowers train MSE: 0.11933059413226069, training time: 856.8542s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 18000, loss: -11.209634, current train MSE: 0.1491886753350946, lowers train MSE: 0.11933059413226069, training time: 998.0259s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 20000, loss: -11.212299, current train MSE: 0.14895596200376532, lowers train MSE: 0.11933059413226069, training time: 1146.2050s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 22000, loss: -11.215375, current train MSE: 0.1482877667204771, lowers train MSE: 0.11933059413226069, training time: 1302.3934s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 24000, loss: -11.217206, current train MSE: 0.1487396947944385, lowers train MSE: 0.11933059413226069, training time: 1468.2390s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 26000, loss: -11.220390, current train MSE: 0.14784626582385554, lowers train MSE: 0.11933059413226069, training time: 1644.2449s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 28000, loss: -11.222306, current train MSE: 0.14809026955628848, lowers train MSE: 0.11933059413226069, training time: 1828.1455s
2000 seconds time exceeded

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 29790, loss: -11.204643, current train MSE: 0.16762481507284638, lowers train MSE: 0.11933059413226069, training time: 2000.4186s
Formula: 0.998*x_2 + 0.494
model's performance on test data:
mse: 32.92416692484905, rmse: 5.737958428295647, mean: 1.540978120893487
std: 5.527441418718163, min: -6.861622318339462, max: 13.895624063637257

iteration: 0, loss: 35.567185, current train MSE: 34.92068813953052, lowers train MSE: 34.92068813953052, training time: 0.1448s
model's performance on test data:
mse: 0.13199724377288316, rmse: 0.36331424933916806, mean: 0.03506064243474113
std: 0.36163665925952326, min: -0.7173488665140351, max: 0.6421757036180455

iteration: 2000, loss: -19.720738, current train MSE: 0.42163384483633887, lowers train MSE: 0.10574793818430331, training time: 143.5648s
model's performance on test data:
mse: 0.13243091812474256, rmse: 0.36391059083893473, mean: 0.028429130812450097
std: 0.362816573933946, min: -0.7120585341214856, max: 0.6588142061180999

iteration: 4000, loss: -21.279889, current train MSE: 0.479390770354599, lowers train MSE: 0.1041490893227476, training time: 292.5298s

##########
reduced lr to 0.0001
##########

Formula: 0.996*x_2 + 0.482*cos(0.041*x_2)
model's performance on test data:
mse: 25.482292232626484, rmse: 5.047998834451776, mean: -0.05869705101044804
std: 5.04790996784698, min: -11.620665445155671, max: 8.995995079027779

iteration: 0, loss: 31.653084, current train MSE: 30.497546308460276, lowers train MSE: 30.497546308460276, training time: 0.1255s
model's performance on test data:
mse: 0.14674568044747285, rmse: 0.3830739882156877, mean: -0.003814755987358848
std: 0.38307414785755495, min: -0.9522503105840308, max: 0.7322301083173706

iteration: 2000, loss: -19.432499, current train MSE: 0.5466535256573288, lowers train MSE: 0.11116922059692204, training time: 144.4179s
model's performance on test data:
mse: 0.07459354520592607, rmse: 0.27311818907924473, mean: 0.09965279076946867
std: 0.2543016584400797, min: -0.6542721511859622, max: 0.6317859741986362

iteration: 4000, loss: -21.413891, current train MSE: 0.25169669959324026, lowers train MSE: 0.07838815255092202, training time: 294.7289s
Formula: 0.013*x_1**2 + 0.001*x_1*x_2 + 1.013*x_2 + 0.016*sin(x_2) + 0.011*sin(0.24*x_1*x_2) - 0.037*sin(0.021*x_1**2 - 0.042*x_2) + 0.011*cos(x_1) - 0.026*cos(x_2) + 0.035*cos(0.24*x_1*x_2) + 0.414*cos(0.021*x_1**2 - 0.042*x_2)
model's performance on test data:
mse: 25.410351878406143, rmse: 5.04086816713214, mean: 0.6502706952036299
std: 4.998999791687242, min: -8.648793799238518, max: 11.598805079536397

iteration: 0, loss: 23.279573, current train MSE: 15.639063395773897, lowers train MSE: 15.639063395773897, training time: 0.7846s
model's performance on test data:
mse: 0.12601606454024622, rmse: 0.35498741462233024, mean: 0.012236775689588796
std: 0.3547941851678663, min: -0.7028511246013665, max: 0.5800438657358082

iteration: 2000, loss: -15.078089, current train MSE: 0.31980304662454107, lowers train MSE: 0.1265101794640851, training time: 145.6940s
model's performance on test data:
mse: 0.12488527828885572, rmse: 0.3533911123512527, mean: -0.01887940949019958
std: 0.3529040951808449, min: -0.7265889328374033, max: 0.5569619336300953

iteration: 4000, loss: -16.249614, current train MSE: 0.2937869765696295, lowers train MSE: 0.11996577826624624, training time: 296.4286s
Formula: 0.976*x_2 + 0.421
model's performance on test data:
mse: 45.369995152516836, rmse: 6.7357252877857805, mean: 0.18631415422883935
std: 6.733484699558362, min: -14.066895265449137, max: 13.956223041961465

iteration: 0, loss: 36.276337, current train MSE: 36.12532402736881, lowers train MSE: 36.12532402736881, training time: 0.7878s
model's performance on test data:
mse: 0.1293525890635344, rmse: 0.3596562095439677, mean: 0.008089257339385581
std: 0.3595832074424471, min: -0.6210134451696803, max: 0.6275115614818034

iteration: 2000, loss: -19.993855, current train MSE: 0.26110694271998947, lowers train MSE: 0.14150719935769768, training time: 146.5049s
model's performance on test data:
mse: 0.13194049299782232, rmse: 0.36323613944350625, mean: 0.0007045109870119519
std: 0.363253619479023, min: -0.6605955602828768, max: 0.655721524288376

iteration: 4000, loss: -21.614119, current train MSE: 0.29820660552881517, lowers train MSE: 0.1263485985426599, training time: 295.8504s
model's performance on test data:
mse: 0.1322625668248353, rmse: 0.36367920867824616, mean: 0.007625787412385731
std: 0.3636174308781159, min: -0.6584701327284854, max: 0.6869134417813783

iteration: 6000, loss: -22.210337, current train MSE: 0.30920756792784754, lowers train MSE: 0.124160161930578, training time: 452.9213s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1322625668248353, rmse: 0.36367920867824616, mean: 0.007625787412385731
std: 0.3636174308781159, min: -0.6584701327284854, max: 0.6869134417813783

iteration: 8000, loss: -22.414655, current train MSE: 0.19027951377862762, lowers train MSE: 0.124160161930578, training time: 617.6712s

##########
reduced lr to 1e-05
##########

Formula: 1.001*x_2 + 0.504
model's performance on test data:
mse: 33.93139833850084, rmse: 5.825066380608966, mean: -0.12474947376261676
std: 5.824021621503713, min: -12.215902688045125, max: 9.315121983007003

iteration: 0, loss: 23.582873, current train MSE: 22.731962840876015, lowers train MSE: 22.731962840876015, training time: 1.2850s
model's performance on test data:
mse: 0.12842692075773043, rmse: 0.3583670196289419, mean: 0.018430603433005045
std: 0.35791066442677394, min: -0.5830054158857525, max: 0.5730492610704481

iteration: 2000, loss: -20.393121, current train MSE: 0.2043961408055771, lowers train MSE: 0.1332246432956068, training time: 146.1017s
model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 4000, loss: -21.846020, current train MSE: 0.17476585688187024, lowers train MSE: 0.1305211075287496, training time: 295.3258s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 6000, loss: -22.163233, current train MSE: 0.17449042431062392, lowers train MSE: 0.1305211075287496, training time: 450.9988s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 8000, loss: -22.198112, current train MSE: 0.1781670508237443, lowers train MSE: 0.1305211075287496, training time: 614.6617s
Formula: 1.001*x_2 + 0.497
model's performance on test data:
mse: 31.07313679826355, rmse: 5.574328371944333, mean: -0.04866249684708118
std: 5.574394690736634, min: -11.237790021118755, max: 9.721325844032481

iteration: 0, loss: 42.083671, current train MSE: 34.372409549397815, lowers train MSE: 34.372409549397815, training time: 1.3354s
model's performance on test data:
mse: 0.19306417048052452, rmse: 0.4393906809213465, mean: 0.16937245918099098
std: 0.40545478172331145, min: -0.9100775667233361, max: 0.9940852150190571

iteration: 2000, loss: -19.129376, current train MSE: 0.5139284459219764, lowers train MSE: 0.2689241740318416, training time: 145.5155s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13325569066574278, rmse: 0.3650420395868711, mean: -0.03187701132526795
std: 0.36366574239203175, min: -0.8363423147382374, max: 0.6437016444570831

iteration: 4000, loss: -20.463964, current train MSE: 0.20340084045293663, lowers train MSE: 0.1749859202034879, training time: 295.4502s
model's performance on test data:
mse: 0.1291942289749067, rmse: 0.35943598731193666, mean: -0.0055973240846698006
std: 0.3594103735508122, min: -0.7429205366644869, max: 0.6024040277529545

iteration: 6000, loss: -20.817204, current train MSE: 0.1936875992173466, lowers train MSE: 0.16101288018714105, training time: 452.7353s
model's performance on test data:
mse: 0.1300460383562403, rmse: 0.3606189656080782, mean: 0.01005327350345688
std: 0.36049683208190064, min: -0.7333385041195175, max: 0.612335158205525

iteration: 8000, loss: -21.109406, current train MSE: 0.21234855543705378, lowers train MSE: 0.15666521199748487, training time: 617.9439s
Formula: 0.996*x_2 + 0.005*cos(x_2) + 0.438
model's performance on test data:
mse: 36.2518963553859, rmse: 6.020954771079575, mean: 0.32462079845388436
std: 6.01249804363706, min: -11.936049390658848, max: 12.38560315459174

iteration: 0, loss: 33.839119, current train MSE: 33.75419841290856, lowers train MSE: 33.75419841290856, training time: 1.3202s