Running on the GPU0

#### starting new seed ####

random seed is: 11135967900375438520
model's performance on test data:
mse: 37.45471084529431, rmse: 6.120025395804686, mean: 0.7243475006679072
std: 6.077312312328884, min: -10.850701972124668, max: 12.303193686718567

iteration: 0, loss: 37.147375, current train MSE: 37.29788113054611, lowers train MSE: 37.29788113054611, training time: 1.0819s
model's performance on test data:
mse: 0.12671417971959573, rmse: 0.3559693522195355, mean: -0.007824295316150419
std: 0.3559011474568284, min: -0.6158739474124202, max: 0.5570815707769157

iteration: 2000, loss: -9.941498, current train MSE: 0.17994217628086961, lowers train MSE: 0.1418457346384858, training time: 84.5275s
model's performance on test data:
mse: 0.14011117980249288, rmse: 0.37431427945309925, mean: 0.08647776740935358
std: 0.36420604090147984, min: -0.6062426579795019, max: 0.7106667672326878

iteration: 4000, loss: -10.543796, current train MSE: 0.2960142350114721, lowers train MSE: 0.1393529343073067, training time: 174.0129s
Formula: 0.966*x_2 + 0.029*sin(x_1) + 0.524
model's performance on test data:
mse: 41.25667258708951, rmse: 6.423135728527734, mean: 1.5770958436353297
std: 6.22682251575457, min: -7.073751490514693, max: 16.362904239750087

iteration: 0, loss: 42.503937, current train MSE: 41.86671570113371, lowers train MSE: 41.86671570113371, training time: 0.0673s
model's performance on test data:
mse: 0.13325654103407364, rmse: 0.3650432043389846, mean: 0.03632357864929192
std: 0.36324968517030554, min: -0.6313209876152834, max: 0.6905411544372217

iteration: 2000, loss: -9.850033, current train MSE: 0.3085009624016992, lowers train MSE: 0.13263531724150102, training time: 82.0280s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13325654103407364, rmse: 0.3650432043389846, mean: 0.03632357864929192
std: 0.36324968517030554, min: -0.6313209876152834, max: 0.6905411544372217

iteration: 4000, loss: -10.288971, current train MSE: 0.17355096646403315, lowers train MSE: 0.13263531724150102, training time: 169.7942s

##########
reduced lr to 1e-05
##########

Formula: 0.998*x_2 + 0.012*sin(x_1) + 0.466
model's performance on test data:
mse: 34.31244771377858, rmse: 5.857682793885188, mean: 1.4456076962690518
std: 5.676785068256721, min: -9.87555698426058, max: 14.496914845099186

iteration: 0, loss: 41.128237, current train MSE: 37.68018105591253, lowers train MSE: 37.68018105591253, training time: 0.4135s
model's performance on test data:
mse: 0.14306317132240165, rmse: 0.3782369248531952, mean: 0.05999134467572674
std: 0.373467746646519, min: -0.9197233520870469, max: 0.7520374590200354

iteration: 2000, loss: -9.115646, current train MSE: 0.5693293496559109, lowers train MSE: 0.1412950531427966, training time: 82.4548s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14306317132240165, rmse: 0.3782369248531952, mean: 0.05999134467572674
std: 0.373467746646519, min: -0.9197233520870469, max: 0.7520374590200354

iteration: 4000, loss: -9.756657, current train MSE: 0.21521745433693165, lowers train MSE: 0.1412950531427966, training time: 171.1077s

##########
reduced lr to 1e-05
##########

Formula: 0.996*x_2 + 0.386
model's performance on test data:
mse: 39.0087420595312, rmse: 6.245697884106403, mean: 1.284963396175988
std: 6.1123929262025944, min: -9.645276392235166, max: 13.634135991994222

iteration: 0, loss: 29.251087, current train MSE: 28.73530306669533, lowers train MSE: 28.73530306669533, training time: 0.0840s
model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 2000, loss: -9.650042, current train MSE: 0.24050046197052202, lowers train MSE: 0.13366474922240146, training time: 81.8516s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 4000, loss: -10.643520, current train MSE: 0.17150098097140637, lowers train MSE: 0.13366474922240146, training time: 170.0596s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 6000, loss: -10.720229, current train MSE: 0.16819637225642348, lowers train MSE: 0.13366474922240146, training time: 266.9677s
model's performance on test data:
mse: 0.12877600648257823, rmse: 0.3588537396803581, mean: 0.004089226100861619
std: 0.35884838302404504, min: -0.7289445004625854, max: 0.5666512585862977

iteration: 8000, loss: -10.731820, current train MSE: 0.16729531010428336, lowers train MSE: 0.13366474922240146, training time: 371.4398s
Formula: 1.0*x_2 + 0.49
model's performance on test data:
mse: 34.86338757624688, rmse: 5.904522637457399, mean: 0.32439150873339767
std: 5.895899753992503, min: -14.417256900992513, max: 15.263414149119988

iteration: 0, loss: 25.687250, current train MSE: 25.298984931432052, lowers train MSE: 25.298984931432052, training time: 0.0827s
model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 2000, loss: -9.437240, current train MSE: 0.27854044557154045, lowers train MSE: 0.12506585134406256, training time: 81.3679s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 4000, loss: -10.468262, current train MSE: 0.17087610234919373, lowers train MSE: 0.12506585134406256, training time: 168.3566s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 6000, loss: -10.551176, current train MSE: 0.16723454465446325, lowers train MSE: 0.12506585134406256, training time: 264.1755s
model's performance on test data:
mse: 0.13240654091901125, rmse: 0.36387709589779244, mean: -0.047035400858431775
std: 0.3608423933098829, min: -0.7972121739381848, max: 0.5272861653361738

iteration: 8000, loss: -10.567824, current train MSE: 0.16332044896982545, lowers train MSE: 0.12506585134406256, training time: 368.5769s
Formula: 0.995*x_2 + 0.489
model's performance on test data:
mse: 34.90426143044104, rmse: 5.907982856308999, mean: 1.5442564417466467
std: 5.702875219666036, min: -8.138267669211764, max: 15.783050697109527

iteration: 0, loss: 41.334886, current train MSE: 37.4833301375671, lowers train MSE: 37.4833301375671, training time: 0.5465s
model's performance on test data:
mse: 0.13453963080562198, rmse: 0.36679644328376737, mean: 0.026854961525629835
std: 0.36583032285055256, min: -0.643073346043165, max: 0.6701227177925198

iteration: 2000, loss: -9.162098, current train MSE: 0.21245885797461353, lowers train MSE: 0.12319483157105215, training time: 81.9531s
model's performance on test data:
mse: 0.13330627680732188, rmse: 0.36511132111634376, mean: 0.0013791123083190934
std: 0.3651269734032412, min: -0.6618894367242039, max: 0.616055867658055

iteration: 4000, loss: -10.252181, current train MSE: 0.32893059045759465, lowers train MSE: 0.11681657563115486, training time: 170.8645s
model's performance on test data:
mse: 0.13730787362495814, rmse: 0.37055077064412933, mean: 0.006598446418123658
std: 0.3705105423829134, min: -0.6933293283353752, max: 0.6567989721380627

iteration: 6000, loss: -10.752544, current train MSE: 0.3540717174885806, lowers train MSE: 0.11373369234683839, training time: 266.2548s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13730787362495814, rmse: 0.37055077064412933, mean: 0.006598446418123658
std: 0.3705105423829134, min: -0.6933293283353752, max: 0.6567989721380627

iteration: 8000, loss: -11.044819, current train MSE: 0.1800848077230915, lowers train MSE: 0.11373369234683839, training time: 369.9169s

##########
reduced lr to 1e-05
##########

Formula: 1.002*x_2 + 0.502
model's performance on test data:
mse: 37.699330761268705, rmse: 6.139978074982736, mean: 0.752240466110674
std: 6.094028124828144, min: -12.375359684889911, max: 14.600118178065403

iteration: 0, loss: 39.592081, current train MSE: 39.80998078195141, lowers train MSE: 39.80998078195141, training time: 0.1087s
model's performance on test data:
mse: 0.13205231851267582, rmse: 0.3633900363420492, mean: 0.001278535352270356
std: 0.3634059580317572, min: -0.8615818113239637, max: 0.6414647474726451

iteration: 2000, loss: -9.825775, current train MSE: 0.25742647929852625, lowers train MSE: 0.13219745324494991, training time: 81.1021s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 4000, loss: -10.457953, current train MSE: 0.3096562950723003, lowers train MSE: 0.12691348942004713, training time: 167.8917s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 6000, loss: -10.846804, current train MSE: 0.16462830696823455, lowers train MSE: 0.12691348942004713, training time: 261.9040s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 8000, loss: -10.877090, current train MSE: 0.1643186331506001, lowers train MSE: 0.12691348942004713, training time: 365.2689s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 10000, loss: -10.885309, current train MSE: 0.1629496901208149, lowers train MSE: 0.12691348942004713, training time: 475.1255s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 12000, loss: -10.890953, current train MSE: 0.16475711455730802, lowers train MSE: 0.12691348942004713, training time: 595.7261s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 14000, loss: -10.897660, current train MSE: 0.16587431715112425, lowers train MSE: 0.12691348942004713, training time: 725.5616s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 16000, loss: -10.908551, current train MSE: 0.16259185127137335, lowers train MSE: 0.12691348942004713, training time: 862.5755s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 18000, loss: -10.913285, current train MSE: 0.16523610643411485, lowers train MSE: 0.12691348942004713, training time: 1007.3626s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 20000, loss: -10.920365, current train MSE: 0.16530193030205798, lowers train MSE: 0.12691348942004713, training time: 1160.0344s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 22000, loss: -10.927333, current train MSE: 0.16526754082923967, lowers train MSE: 0.12691348942004713, training time: 1320.6866s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 24000, loss: -10.934111, current train MSE: 0.1652446290140634, lowers train MSE: 0.12691348942004713, training time: 1492.1524s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 26000, loss: -10.940844, current train MSE: 0.16508793802892274, lowers train MSE: 0.12691348942004713, training time: 1673.4353s
model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 28000, loss: -10.947275, current train MSE: 0.16506482648033444, lowers train MSE: 0.12691348942004713, training time: 1862.5510s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13114979957596087, rmse: 0.3621461025276413, mean: 0.026463763412056245
std: 0.36119595116442055, min: -0.7781399462940062, max: 0.635503289838923

iteration: 29410, loss: -10.955111, current train MSE: 0.16163523713317995, lowers train MSE: 0.12691348942004713, training time: 2000.8547s
Formula: 1.008*x_2 + 0.492
model's performance on test data:
mse: 26.177243346449444, rmse: 5.116370133839951, mean: 0.3996858638204521
std: 5.100989764720858, min: -9.280107345171448, max: 10.125835585409915

iteration: 0, loss: 23.583269, current train MSE: 23.624221876423196, lowers train MSE: 23.624221876423196, training time: 0.1068s
model's performance on test data:
mse: 0.13133947474746913, rmse: 0.3624078844995913, mean: 0.03633270792150655
std: 0.36060007267018207, min: -0.724822827183349, max: 0.6277924032530242

iteration: 2000, loss: -9.943806, current train MSE: 0.26236146286287954, lowers train MSE: 0.12876434630136674, training time: 80.8181s
model's performance on test data:
mse: 0.13216815335330417, rmse: 0.36354938227605915, mean: 0.02445699357879713
std: 0.36274394167453117, min: -0.7555586649508736, max: 0.626571355421393

iteration: 4000, loss: -10.666364, current train MSE: 0.1907389872044917, lowers train MSE: 0.12429969575316341, training time: 167.1301s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 6000, loss: -10.872722, current train MSE: 0.3087170500374795, lowers train MSE: 0.11982482445382381, training time: 261.4395s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 8000, loss: -11.094021, current train MSE: 0.19016639388173734, lowers train MSE: 0.11982482445382381, training time: 364.6498s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 10000, loss: -11.106212, current train MSE: 0.19104853762339835, lowers train MSE: 0.11982482445382381, training time: 475.5214s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 12000, loss: -11.112123, current train MSE: 0.18866112258609447, lowers train MSE: 0.11982482445382381, training time: 592.3827s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 14000, loss: -11.116055, current train MSE: 0.18851374929143408, lowers train MSE: 0.11982482445382381, training time: 719.8290s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 16000, loss: -11.119833, current train MSE: 0.18841080676192817, lowers train MSE: 0.11982482445382381, training time: 854.1838s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 18000, loss: -11.124132, current train MSE: 0.18767925686109402, lowers train MSE: 0.11982482445382381, training time: 997.2077s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 20000, loss: -11.126282, current train MSE: 0.1889717755661139, lowers train MSE: 0.11982482445382381, training time: 1148.4917s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 22000, loss: -11.131449, current train MSE: 0.18715408447479392, lowers train MSE: 0.11982482445382381, training time: 1307.7220s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 24000, loss: -11.134406, current train MSE: 0.1874665799182336, lowers train MSE: 0.11982482445382381, training time: 1476.9938s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 26000, loss: -11.137451, current train MSE: 0.18761556945251692, lowers train MSE: 0.11982482445382381, training time: 1660.9297s
model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 28000, loss: -11.141108, current train MSE: 0.18708805263411113, lowers train MSE: 0.11982482445382381, training time: 1851.1446s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13375189130988901, rmse: 0.36572105669470145, mean: 0.024045246772685085
std: 0.36494799110120973, min: -0.7779969820044759, max: 0.6373171691093864

iteration: 29510, loss: -11.143675, current train MSE: 0.1868417213267827, lowers train MSE: 0.11982482445382381, training time: 2000.3108s
Formula: 1.0*x_2 + 0.495
model's performance on test data:
mse: 31.433775601213426, rmse: 5.60658323769597, mean: -0.10841820170966648
std: 5.605815162307112, min: -11.131717570435717, max: 11.062330770376128

iteration: 0, loss: 33.436509, current train MSE: 29.65310470010033, lowers train MSE: 29.65310470010033, training time: 1.4217s
model's performance on test data:
mse: 0.1510690207715723, rmse: 0.388675984299998, mean: 0.07567255004552738
std: 0.38125742188920364, min: -0.9571691123450456, max: 0.8138475263353893

iteration: 2000, loss: -8.470937, current train MSE: 0.5847871174040729, lowers train MSE: 0.1995571519256292, training time: 80.5076s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 4000, loss: -9.867337, current train MSE: 0.4413355584594274, lowers train MSE: 0.12462668623188607, training time: 166.6545s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 6000, loss: -10.717756, current train MSE: 0.16389733206272877, lowers train MSE: 0.12462668623188607, training time: 260.5941s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 8000, loss: -10.772285, current train MSE: 0.16351437876936725, lowers train MSE: 0.12462668623188607, training time: 363.1867s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 10000, loss: -10.779462, current train MSE: 0.1647086005304877, lowers train MSE: 0.12462668623188607, training time: 473.1212s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 12000, loss: -10.785591, current train MSE: 0.1680251310637463, lowers train MSE: 0.12462668623188607, training time: 591.0033s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 14000, loss: -10.800190, current train MSE: 0.16353198856018353, lowers train MSE: 0.12462668623188607, training time: 718.5897s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 16000, loss: -10.812976, current train MSE: 0.1604934916428257, lowers train MSE: 0.12462668623188607, training time: 853.0328s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 18000, loss: -10.821834, current train MSE: 0.16115108456761928, lowers train MSE: 0.12462668623188607, training time: 995.3396s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 20000, loss: -10.832280, current train MSE: 0.16010803798792153, lowers train MSE: 0.12462668623188607, training time: 1145.5564s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 22000, loss: -10.837283, current train MSE: 0.16412041102471286, lowers train MSE: 0.12462668623188607, training time: 1303.3740s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 24000, loss: -10.846338, current train MSE: 0.16390943549181158, lowers train MSE: 0.12462668623188607, training time: 1470.4375s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 26000, loss: -10.857309, current train MSE: 0.1615700705319778, lowers train MSE: 0.12462668623188607, training time: 1648.1476s
model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 28000, loss: -10.865695, current train MSE: 0.16162803041792412, lowers train MSE: 0.12462668623188607, training time: 1835.0971s
2000 seconds time exceeded

model's performance on test data:
mse: 0.14310785333824141, rmse: 0.37829598641571843, mean: 0.07008812534682647
std: 0.3717651530665488, min: -0.7737451890282205, max: 0.7460435446315561

iteration: 29690, loss: -10.877235, current train MSE: 0.1572410817557231, lowers train MSE: 0.12462668623188607, training time: 2000.6963s
Formula: 1.0*x_2 + 0.495*cos(0.058*x_2)
model's performance on test data:
mse: 31.28082757860165, rmse: 5.59292656653041, mean: 0.29357541363177697
std: 5.585495577577269, min: -10.778585389786628, max: 10.47168909776954

iteration: 0, loss: 27.604341, current train MSE: 27.50728528719557, lowers train MSE: 27.50728528719557, training time: 0.0837s
model's performance on test data:
mse: 0.1403487651943619, rmse: 0.3746315058752559, mean: 0.04514759456488196
std: 0.37191973919261634, min: -0.9189460001305942, max: 0.7677000506670821

iteration: 2000, loss: -9.456577, current train MSE: 0.43944413033223917, lowers train MSE: 0.16714082781571152, training time: 80.5473s
model's performance on test data:
mse: 0.13564775295302856, rmse: 0.3683038866928077, mean: -0.007858964063253355
std: 0.368238441284421, min: -0.7924723214874128, max: 0.634540352503679

iteration: 4000, loss: -10.390025, current train MSE: 0.3897952917733898, lowers train MSE: 0.10723286026536519, training time: 166.9627s
Formula: 1.06*x_2 + 0.451
model's performance on test data:
mse: 33.81027581867358, rmse: 5.814660421613079, mean: 0.3558871901500577
std: 5.804049349214654, min: -12.284846055273729, max: 14.258755784749308

iteration: 0, loss: 32.678529, current train MSE: 32.74746727757576, lowers train MSE: 32.74746727757576, training time: 0.0808s
model's performance on test data:
mse: 0.1311897500877052, rmse: 0.3622012563309316, mean: 0.02477856165213499
std: 0.36137076794608136, min: -0.6627261493265326, max: 0.6812516618664155

iteration: 2000, loss: -10.013976, current train MSE: 0.16873396185074063, lowers train MSE: 0.1309767918849672, training time: 79.7830s
model's performance on test data:
mse: 0.13432040695625216, rmse: 0.36649748560699863, mean: 0.03346753124329169
std: 0.36498445548815334, min: -0.6729278650870452, max: 0.6940085193106018

iteration: 4000, loss: -10.664859, current train MSE: 0.15673714734839322, lowers train MSE: 0.1282540353378009, training time: 165.6752s
Formula: 0.963*x_2 - 0.039*cos(x_1) + 0.409
model's performance on test data:
mse: 43.58491829659982, rmse: 6.601887479849972, mean: 0.286783948995869
std: 6.5959854452966855, min: -13.38784586710791, max: 12.258763756067635

iteration: 0, loss: 70.646412, current train MSE: 66.79271983275915, lowers train MSE: 66.79271983275915, training time: 0.0791s
model's performance on test data:
mse: 0.1504246310603237, rmse: 0.3878461435418995, mean: 0.05763234805797426
std: 0.38355945485482523, min: -0.8296244636598455, max: 0.7966713380026125

iteration: 2000, loss: -8.643030, current train MSE: 0.17727067994901904, lowers train MSE: 0.124644519188442, training time: 79.9660s
model's performance on test data:
mse: 0.13814381472294926, rmse: 0.37167703012555037, mean: -0.027238314846614074
std: 0.3706961432036334, min: -0.7373474786478962, max: 0.672059029878989

iteration: 4000, loss: -10.061888, current train MSE: 0.37863252188234925, lowers train MSE: 0.12091969718584239, training time: 166.1007s

##########
reduced lr to 0.0001
##########

Formula: 1.008*x_2 + 0.495
model's performance on test data:
mse: 39.31414653500765, rmse: 6.270099403917585, mean: 1.673138732417123
std: 6.043046018161045, min: -11.17016306671892, max: 18.61646942178563

iteration: 0, loss: 49.001638, current train MSE: 49.140778237645044, lowers train MSE: 49.140778237645044, training time: 0.4056s
model's performance on test data:
mse: 0.13130333562843402, rmse: 0.36235802133861206, mean: 0.053480738769945406
std: 0.3584075778883216, min: -0.7138144818788117, max: 0.6730542103721682

iteration: 2000, loss: -9.291553, current train MSE: 0.17699219764506213, lowers train MSE: 0.12325100318604139, training time: 80.4417s
model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 4000, loss: -10.114221, current train MSE: 0.5083400320178373, lowers train MSE: 0.10598698884904828, training time: 166.6096s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 6000, loss: -10.895455, current train MSE: 0.2007119651764338, lowers train MSE: 0.10598698884904828, training time: 260.7771s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.15359605480414054, rmse: 0.39191332562716025, mean: 0.05145970523266183
std: 0.38853963753027215, min: -0.9117959561215407, max: 0.8421581634047399

iteration: 8000, loss: -10.961466, current train MSE: 0.1773583981716092, lowers train MSE: 0.10598698884904828, training time: 363.3315s
Formula: -0.004*x_1 + 0.999*x_2 + 0.502
model's performance on test data:
mse: 33.43556321911492, rmse: 5.782349282005967, mean: 0.919785905865466
std: 5.709011859140262, min: -8.873012605263837, max: 11.999940055058907

iteration: 0, loss: 27.727467, current train MSE: 27.26097362396836, lowers train MSE: 27.26097362396836, training time: 0.0885s
model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 2000, loss: -9.852144, current train MSE: 0.1595109437239794, lowers train MSE: 0.12238034580480973, training time: 79.9273s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 4000, loss: -10.223362, current train MSE: 0.15915216913088343, lowers train MSE: 0.12238034580480973, training time: 166.3522s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 6000, loss: -10.259859, current train MSE: 0.16148187657360175, lowers train MSE: 0.12238034580480973, training time: 262.2626s
model's performance on test data:
mse: 0.12863065111938615, rmse: 0.358651155190369, mean: -0.003966286239583018
std: 0.3586471560757553, min: -0.6682762496606092, max: 0.5933397065273578

iteration: 8000, loss: -10.280509, current train MSE: 0.1625284450583011, lowers train MSE: 0.12238034580480973, training time: 365.3991s
Formula: 0.992*x_2 + 0.497
model's performance on test data:
mse: 33.897728989359884, rmse: 5.8221756233696595, mean: 0.965553300046171
std: 5.741840533048874, min: -10.275966716691537, max: 12.898155869068502

iteration: 0, loss: 35.176470, current train MSE: 31.847720090609897, lowers train MSE: 31.847720090609897, training time: 0.6556s
model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 2000, loss: -9.168069, current train MSE: 0.39372344954809746, lowers train MSE: 0.11067269318254061, training time: 81.1002s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 4000, loss: -10.511758, current train MSE: 0.16904086560859066, lowers train MSE: 0.11067269318254061, training time: 167.0637s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 6000, loss: -10.598508, current train MSE: 0.17086150907282982, lowers train MSE: 0.11067269318254061, training time: 260.9887s
model's performance on test data:
mse: 0.135533525393705, rmse: 0.3681487816001908, mean: 0.02355789782257336
std: 0.3674126428691373, min: -0.6763229880283053, max: 0.6600337704056702

iteration: 8000, loss: -10.612454, current train MSE: 0.16871257790388539, lowers train MSE: 0.11067269318254061, training time: 363.4290s
Formula: 1.004*x_2 + 0.5
model's performance on test data:
mse: 30.06686197502257, rmse: 5.483325813320103, mean: -0.03975748888116929
std: 5.483455859810432, min: -11.504065782554813, max: 9.505307494121128

iteration: 0, loss: 25.169219, current train MSE: 25.41213775465784, lowers train MSE: 25.41213775465784, training time: 0.6932s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 2000, loss: -9.173949, current train MSE: 0.28338096399422824, lowers train MSE: 0.11264185686582269, training time: 81.3573s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 4000, loss: -10.277914, current train MSE: 0.17477391839556514, lowers train MSE: 0.11264185686582269, training time: 167.7123s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 6000, loss: -10.385690, current train MSE: 0.16198268178976663, lowers train MSE: 0.11264185686582269, training time: 261.4654s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 8000, loss: -10.407247, current train MSE: 0.1556099371493551, lowers train MSE: 0.11264185686582269, training time: 365.9929s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 10000, loss: -10.420168, current train MSE: 0.15959530079555342, lowers train MSE: 0.11264185686582269, training time: 477.3327s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 12000, loss: -10.439760, current train MSE: 0.1570754537774507, lowers train MSE: 0.11264185686582269, training time: 595.0646s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 14000, loss: -10.455421, current train MSE: 0.15807604287168514, lowers train MSE: 0.11264185686582269, training time: 721.8713s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 16000, loss: -10.471918, current train MSE: 0.15749002442304982, lowers train MSE: 0.11264185686582269, training time: 856.6030s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 18000, loss: -10.485260, current train MSE: 0.15952118831849682, lowers train MSE: 0.11264185686582269, training time: 998.5607s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 20000, loss: -10.499917, current train MSE: 0.15954475442056198, lowers train MSE: 0.11264185686582269, training time: 1148.0591s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 22000, loss: -10.515648, current train MSE: 0.15831362780785108, lowers train MSE: 0.11264185686582269, training time: 1306.1058s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 24000, loss: -10.530554, current train MSE: 0.15742955288656257, lowers train MSE: 0.11264185686582269, training time: 1473.3974s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 26000, loss: -10.542786, current train MSE: 0.15895756304171665, lowers train MSE: 0.11264185686582269, training time: 1651.8730s
model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 28000, loss: -10.553310, current train MSE: 0.161882149564666, lowers train MSE: 0.11264185686582269, training time: 1837.6859s
2000 seconds time exceeded

model's performance on test data:
mse: 0.14738540731549696, rmse: 0.3839080714383288, mean: 0.009964184297558269
std: 0.38379793192709927, min: -0.8313038979867926, max: 0.7546017973001202

iteration: 29670, loss: -10.546224, current train MSE: 0.17986839944706592, lowers train MSE: 0.11264185686582269, training time: 2000.5227s
Formula: -0.006*x_1 + 0.991*x_2 + 0.062*sin(0.041*x_1 + 0.041*x_2) + 0.449*cos(0.041*x_1 + 0.041*x_2) + 0.048
model's performance on test data:
mse: 38.38875366979218, rmse: 6.195865853114654, mean: 1.524688708816288
std: 6.005637751812585, min: -10.382761669700095, max: 14.031421058144113

iteration: 0, loss: 36.962234, current train MSE: 36.79381789347789, lowers train MSE: 36.79381789347789, training time: 0.1121s
model's performance on test data:
mse: 0.12870386821661672, rmse: 0.35875321352793027, mean: -0.009932561262568319
std: 0.3586336213271653, min: -0.6343853872734124, max: 0.6185774542741029

iteration: 2000, loss: -9.870931, current train MSE: 0.1677073737049436, lowers train MSE: 0.12551641524020798, training time: 80.8646s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 4000, loss: -10.487863, current train MSE: 0.3074774948184264, lowers train MSE: 0.1163184025182096, training time: 167.4068s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 6000, loss: -10.936473, current train MSE: 0.19184686509313634, lowers train MSE: 0.1163184025182096, training time: 261.5011s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 8000, loss: -10.980923, current train MSE: 0.1796865418740439, lowers train MSE: 0.1163184025182096, training time: 363.9677s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 10000, loss: -10.987395, current train MSE: 0.1780667942731924, lowers train MSE: 0.1163184025182096, training time: 473.4206s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 12000, loss: -10.992901, current train MSE: 0.1781528553757266, lowers train MSE: 0.1163184025182096, training time: 590.8469s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 14000, loss: -10.999009, current train MSE: 0.17783472231907962, lowers train MSE: 0.1163184025182096, training time: 717.4657s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 16000, loss: -11.004507, current train MSE: 0.1780729374256778, lowers train MSE: 0.1163184025182096, training time: 852.3041s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 18000, loss: -11.010143, current train MSE: 0.1780185888115269, lowers train MSE: 0.1163184025182096, training time: 994.4046s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 20000, loss: -11.015752, current train MSE: 0.17781170184986156, lowers train MSE: 0.1163184025182096, training time: 1145.0210s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 22000, loss: -11.020652, current train MSE: 0.1781428988374986, lowers train MSE: 0.1163184025182096, training time: 1304.9611s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 24000, loss: -11.025704, current train MSE: 0.17816857972975103, lowers train MSE: 0.1163184025182096, training time: 1473.3324s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 26000, loss: -11.030815, current train MSE: 0.1779984366961111, lowers train MSE: 0.1163184025182096, training time: 1650.0545s
model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 28000, loss: -11.035515, current train MSE: 0.17810390303432813, lowers train MSE: 0.1163184025182096, training time: 1835.8030s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13226796916380568, rmse: 0.3636866359433705, mean: 0.013161729942418697
std: 0.363466571941402, min: -0.6421340754924927, max: 0.6103501230708073

iteration: 29700, loss: -11.039486, current train MSE: 0.17812142465047975, lowers train MSE: 0.1163184025182096, training time: 2000.1501s
Formula: 1.0*x_2 + 0.497
model's performance on test data:
mse: 51.62797057078056, rmse: 7.185260647379506, mean: -2.2761354450085443
std: 6.81555743966151, min: -20.336238163279482, max: 5.381503464370757

iteration: 0, loss: 34.792433, current train MSE: 31.501043969368993, lowers train MSE: 31.501043969368993, training time: 0.1259s
model's performance on test data:
mse: 0.1332012517787885, rmse: 0.36496746674024044, mean: 0.07977495143457808
std: 0.3561599275006231, min: -0.5450062823483481, max: 0.6270494762474037

iteration: 2000, loss: -9.702247, current train MSE: 0.29805097450211254, lowers train MSE: 0.12877807262012847, training time: 80.5217s
model's performance on test data:
mse: 0.13628162329481672, rmse: 0.36916341001623754, mean: 0.007192684019895701
std: 0.36911178932360356, min: -0.6995627448992305, max: 0.646647965812635

iteration: 4000, loss: -10.585455, current train MSE: 0.22928770822052985, lowers train MSE: 0.12374682708727891, training time: 166.9631s
model's performance on test data:
mse: 0.12917123108116887, rmse: 0.3594039942476556, mean: 0.03347079696952259
std: 0.3578599492421924, min: -0.5905309464042574, max: 0.6057329993781089

iteration: 6000, loss: -10.882002, current train MSE: 0.27639260903102214, lowers train MSE: 0.12203019090432439, training time: 261.7456s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 8000, loss: -11.051602, current train MSE: 0.27691050900030323, lowers train MSE: 0.11933059413226069, training time: 365.2443s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 10000, loss: -11.201210, current train MSE: 0.14711622714675068, lowers train MSE: 0.11933059413226069, training time: 476.0704s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 12000, loss: -11.200007, current train MSE: 0.15135773625936325, lowers train MSE: 0.11933059413226069, training time: 594.4459s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 14000, loss: -11.203195, current train MSE: 0.15060843151514575, lowers train MSE: 0.11933059413226069, training time: 722.1485s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 16000, loss: -11.206699, current train MSE: 0.1495173378314704, lowers train MSE: 0.11933059413226069, training time: 857.2142s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 18000, loss: -11.209634, current train MSE: 0.1491886753350946, lowers train MSE: 0.11933059413226069, training time: 999.3777s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 20000, loss: -11.212299, current train MSE: 0.14895596200376532, lowers train MSE: 0.11933059413226069, training time: 1148.7225s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 22000, loss: -11.215375, current train MSE: 0.1482877667204771, lowers train MSE: 0.11933059413226069, training time: 1305.6182s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 24000, loss: -11.217206, current train MSE: 0.1487396947944385, lowers train MSE: 0.11933059413226069, training time: 1472.2856s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 26000, loss: -11.220390, current train MSE: 0.14784626582385554, lowers train MSE: 0.11933059413226069, training time: 1649.4331s
model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 28000, loss: -11.222306, current train MSE: 0.14809026955628848, lowers train MSE: 0.11933059413226069, training time: 1834.6512s
2000 seconds time exceeded

model's performance on test data:
mse: 0.1263365022847514, rmse: 0.35543846483568914, mean: -0.0055104683287804
std: 0.35541351826114703, min: -0.5832562666355052, max: 0.5222187272802374

iteration: 29710, loss: -11.210501, current train MSE: 0.16194342661318398, lowers train MSE: 0.11933059413226069, training time: 2000.0105s
Formula: 1.001*x_2 + 0.491
model's performance on test data:
mse: 32.92416692484905, rmse: 5.737958428295647, mean: 1.540978120893487
std: 5.527441418718163, min: -6.861622318339462, max: 13.895624063637257

iteration: 0, loss: 35.567185, current train MSE: 34.92068813953052, lowers train MSE: 34.92068813953052, training time: 0.1334s
model's performance on test data:
mse: 0.13199724377288316, rmse: 0.36331424933916806, mean: 0.03506064243474113
std: 0.36163665925952326, min: -0.7173488665140351, max: 0.6421757036180455

iteration: 2000, loss: -19.720738, current train MSE: 0.42163384483633887, lowers train MSE: 0.10574793818430331, training time: 143.0019s
model's performance on test data:
mse: 0.13243091812474256, rmse: 0.36391059083893473, mean: 0.028429130812450097
std: 0.362816573933946, min: -0.7120585341214856, max: 0.6588142061180999

iteration: 4000, loss: -21.279889, current train MSE: 0.479390770354599, lowers train MSE: 0.1041490893227476, training time: 291.0871s

##########
reduced lr to 0.0001
##########

Formula: 0.996*x_2 + 0.482*cos(0.041*x_2)
model's performance on test data:
mse: 25.482292232626484, rmse: 5.047998834451776, mean: -0.05869705101044804
std: 5.04790996784698, min: -11.620665445155671, max: 8.995995079027779

iteration: 0, loss: 31.653084, current train MSE: 30.497546308460276, lowers train MSE: 30.497546308460276, training time: 0.1226s
model's performance on test data:
mse: 0.14674568044747285, rmse: 0.3830739882156877, mean: -0.003814755987358848
std: 0.38307414785755495, min: -0.9522503105840308, max: 0.7322301083173706

iteration: 2000, loss: -19.432499, current train MSE: 0.5466535256573288, lowers train MSE: 0.11116922059692204, training time: 142.2953s
model's performance on test data:
mse: 0.07459354520592607, rmse: 0.27311818907924473, mean: 0.09965279076946867
std: 0.2543016584400797, min: -0.6542721511859622, max: 0.6317859741986362

iteration: 4000, loss: -21.413891, current train MSE: 0.25169669959324026, lowers train MSE: 0.07838815255092202, training time: 291.1064s
Formula: 0.013*x_1**2 + 0.001*x_1*x_2 + 1.013*x_2 + 0.016*sin(x_2) + 0.011*sin(0.24*x_1*x_2) - 0.037*sin(0.021*x_1**2 - 0.042*x_2) + 0.011*cos(x_1) - 0.026*cos(x_2) + 0.035*cos(0.24*x_1*x_2) + 0.414*cos(0.021*x_1**2 - 0.042*x_2)
model's performance on test data:
mse: 25.410351878406143, rmse: 5.04086816713214, mean: 0.6502706952036299
std: 4.998999791687242, min: -8.648793799238518, max: 11.598805079536397

iteration: 0, loss: 23.279573, current train MSE: 15.639063395773897, lowers train MSE: 15.639063395773897, training time: 0.7716s
model's performance on test data:
mse: 0.12601606454024622, rmse: 0.35498741462233024, mean: 0.012236775689588796
std: 0.3547941851678663, min: -0.7028511246013665, max: 0.5800438657358082

iteration: 2000, loss: -15.078089, current train MSE: 0.31980304662454107, lowers train MSE: 0.1265101794640851, training time: 145.3366s
model's performance on test data:
mse: 0.12488527828885572, rmse: 0.3533911123512527, mean: -0.01887940949019958
std: 0.3529040951808449, min: -0.7265889328374033, max: 0.5569619336300953

iteration: 4000, loss: -16.249614, current train MSE: 0.2937869765696295, lowers train MSE: 0.11996577826624624, training time: 294.8301s
Formula: 0.976*x_2 + 0.421
model's performance on test data:
mse: 45.369995152516836, rmse: 6.7357252877857805, mean: 0.18631415422883935
std: 6.733484699558362, min: -14.066895265449137, max: 13.956223041961465

iteration: 0, loss: 36.276337, current train MSE: 36.12532402736881, lowers train MSE: 36.12532402736881, training time: 0.7803s
model's performance on test data:
mse: 0.1293525890635344, rmse: 0.3596562095439677, mean: 0.008089257339385581
std: 0.3595832074424471, min: -0.6210134451696803, max: 0.6275115614818034

iteration: 2000, loss: -19.993855, current train MSE: 0.26110694271998947, lowers train MSE: 0.14150719935769768, training time: 145.4703s
model's performance on test data:
mse: 0.13194049299782232, rmse: 0.36323613944350625, mean: 0.0007045109870119519
std: 0.363253619479023, min: -0.6605955602828768, max: 0.655721524288376

iteration: 4000, loss: -21.614119, current train MSE: 0.29820660552881517, lowers train MSE: 0.1263485985426599, training time: 294.7696s
model's performance on test data:
mse: 0.1322625668248353, rmse: 0.36367920867824616, mean: 0.007625787412385731
std: 0.3636174308781159, min: -0.6584701327284854, max: 0.6869134417813783

iteration: 6000, loss: -22.210337, current train MSE: 0.30920756792784754, lowers train MSE: 0.124160161930578, training time: 451.1608s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1322625668248353, rmse: 0.36367920867824616, mean: 0.007625787412385731
std: 0.3636174308781159, min: -0.6584701327284854, max: 0.6869134417813783

iteration: 8000, loss: -22.414655, current train MSE: 0.19027951377862762, lowers train MSE: 0.124160161930578, training time: 614.7641s

##########
reduced lr to 1e-05
##########

Formula: 1.001*x_2 + 0.504
model's performance on test data:
mse: 33.93139833850084, rmse: 5.825066380608966, mean: -0.12474947376261676
std: 5.824021621503713, min: -12.215902688045125, max: 9.315121983007003

iteration: 0, loss: 23.582873, current train MSE: 22.731962840876015, lowers train MSE: 22.731962840876015, training time: 1.3047s
model's performance on test data:
mse: 0.12842692075773043, rmse: 0.3583670196289419, mean: 0.018430603433005045
std: 0.35791066442677394, min: -0.5830054158857525, max: 0.5730492610704481

iteration: 2000, loss: -20.393121, current train MSE: 0.2043961408055771, lowers train MSE: 0.1332246432956068, training time: 145.5069s
model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 4000, loss: -21.846020, current train MSE: 0.17476585688187024, lowers train MSE: 0.1305211075287496, training time: 293.4458s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 6000, loss: -22.163233, current train MSE: 0.17449042431062392, lowers train MSE: 0.1305211075287496, training time: 449.1466s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1284312889846157, rmse: 0.3583731142044778, mean: 0.010891565350549179
std: 0.35822548126843845, min: -0.5921694696358202, max: 0.5771128316200951

iteration: 8000, loss: -22.198112, current train MSE: 0.1781670508237443, lowers train MSE: 0.1305211075287496, training time: 612.9165s
Formula: 1.001*x_2 + 0.497
model's performance on test data:
mse: 31.07313679826355, rmse: 5.574328371944333, mean: -0.04866249684708118
std: 5.574394690736634, min: -11.237790021118755, max: 9.721325844032481

iteration: 0, loss: 42.083671, current train MSE: 34.372409549397815, lowers train MSE: 34.372409549397815, training time: 1.2824s
model's performance on test data:
mse: 0.19306417048052452, rmse: 0.4393906809213465, mean: 0.16937245918099098
std: 0.40545478172331145, min: -0.9100775667233361, max: 0.9940852150190571

iteration: 2000, loss: -19.129376, current train MSE: 0.5139284459219764, lowers train MSE: 0.2689241740318416, training time: 143.9918s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13325569066574278, rmse: 0.3650420395868711, mean: -0.03187701132526795
std: 0.36366574239203175, min: -0.8363423147382374, max: 0.6437016444570831

iteration: 4000, loss: -20.463964, current train MSE: 0.20340084045293663, lowers train MSE: 0.1749859202034879, training time: 294.0082s
model's performance on test data:
mse: 0.1291942289749067, rmse: 0.35943598731193666, mean: -0.0055973240846698006
std: 0.3594103735508122, min: -0.7429205366644869, max: 0.6024040277529545

iteration: 6000, loss: -20.817204, current train MSE: 0.1936875992173466, lowers train MSE: 0.16101288018714105, training time: 450.3402s
model's performance on test data:
mse: 0.1300460383562403, rmse: 0.3606189656080782, mean: 0.01005327350345688
std: 0.36049683208190064, min: -0.7333385041195175, max: 0.612335158205525

iteration: 8000, loss: -21.109406, current train MSE: 0.21234855543705378, lowers train MSE: 0.15666521199748487, training time: 617.7580s
Formula: 0.996*x_2 + 0.005*cos(x_2) + 0.438
model's performance on test data:
mse: 36.2518963553859, rmse: 6.020954771079575, mean: 0.32462079845388436
std: 6.01249804363706, min: -11.936049390658848, max: 12.38560315459174

iteration: 0, loss: 33.839119, current train MSE: 33.75419841290856, lowers train MSE: 33.75419841290856, training time: 1.3196s
model's performance on test data:
mse: 0.1301655259500951, rmse: 0.3607845977173847, mean: -0.023526317553121914
std: 0.3600347218224872, min: -0.6423227978847121, max: 0.6292854070658702

iteration: 2000, loss: -20.190692, current train MSE: 0.2692893022256088, lowers train MSE: 0.11940342416578155, training time: 144.8238s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 4000, loss: -21.649282, current train MSE: 0.3138068299420097, lowers train MSE: 0.10482558874148636, training time: 294.0093s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 6000, loss: -22.158751, current train MSE: 0.17542451818542318, lowers train MSE: 0.10482558874148636, training time: 450.8537s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 8000, loss: -22.201271, current train MSE: 0.17633364959819137, lowers train MSE: 0.10482558874148636, training time: 615.2935s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 10000, loss: -22.213937, current train MSE: 0.1741051952789534, lowers train MSE: 0.10482558874148636, training time: 787.3242s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 12000, loss: -22.225699, current train MSE: 0.17344851242247827, lowers train MSE: 0.10482558874148636, training time: 965.9046s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 14000, loss: -22.233386, current train MSE: 0.1767853411095655, lowers train MSE: 0.10482558874148636, training time: 1154.6964s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 16000, loss: -22.243854, current train MSE: 0.1770941599322582, lowers train MSE: 0.10482558874148636, training time: 1353.8928s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 18000, loss: -22.256479, current train MSE: 0.17493292514791028, lowers train MSE: 0.10482558874148636, training time: 1562.7090s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 20000, loss: -22.266238, current train MSE: 0.17540754168291, lowers train MSE: 0.10482558874148636, training time: 1776.7535s
model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 22000, loss: -22.277666, current train MSE: 0.17392970999608887, lowers train MSE: 0.10482558874148636, training time: 1998.8604s
2000 seconds time exceeded

model's performance on test data:
mse: 0.1344655346118304, rmse: 0.3666954248580563, mean: 0.013654617646508832
std: 0.3664594321021557, min: -0.8138170018804036, max: 0.6877377428561822

iteration: 22010, loss: -22.275658, current train MSE: 0.17598643348223963, lowers train MSE: 0.10482558874148636, training time: 2000.1489s
Formula: 0.997*x_2 + 0.498
model's performance on test data:
mse: 30.5028535951269, rmse: 5.522938854914736, mean: 1.0253512936805391
std: 5.427195757183612, min: -8.707916529128115, max: 11.235894880054706

iteration: 0, loss: 25.526156, current train MSE: 25.636973521496067, lowers train MSE: 25.636973521496067, training time: 0.1712s
model's performance on test data:
mse: 0.13283837059864104, rmse: 0.3644699858680287, mean: 0.012505576771080558
std: 0.364273593281108, min: -0.7011751004058944, max: 0.6442936147561955

iteration: 2000, loss: -20.424504, current train MSE: 0.18909775871661097, lowers train MSE: 0.14376391376250555, training time: 143.8353s
model's performance on test data:
mse: 0.1311382873882094, rmse: 0.36213020778196536, mean: -0.028186378866094278
std: 0.36104965199700334, min: -0.7052548262314211, max: 0.5808655773815854

iteration: 4000, loss: -21.804486, current train MSE: 0.233533176863908, lowers train MSE: 0.1422701075112881, training time: 292.8974s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 6000, loss: -22.411068, current train MSE: 0.15625678149243336, lowers train MSE: 0.13606419909623213, training time: 449.1012s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 8000, loss: -22.462216, current train MSE: 0.17661589690850749, lowers train MSE: 0.13606419909623213, training time: 614.2748s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 10000, loss: -22.477335, current train MSE: 0.17169793310603745, lowers train MSE: 0.13606419909623213, training time: 785.6955s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 12000, loss: -22.484399, current train MSE: 0.17137426410322124, lowers train MSE: 0.13606419909623213, training time: 965.6546s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 14000, loss: -22.491762, current train MSE: 0.17104253198144248, lowers train MSE: 0.13606419909623213, training time: 1155.3296s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 16000, loss: -22.497723, current train MSE: 0.17197141514851935, lowers train MSE: 0.13606419909623213, training time: 1352.5500s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 18000, loss: -22.506090, current train MSE: 0.17028142128011986, lowers train MSE: 0.13606419909623213, training time: 1557.5586s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 20000, loss: -22.511425, current train MSE: 0.17140695018640395, lowers train MSE: 0.13606419909623213, training time: 1769.9953s
model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 22000, loss: -22.519669, current train MSE: 0.16944654985845775, lowers train MSE: 0.13606419909623213, training time: 1992.9210s
2000 seconds time exceeded

model's performance on test data:
mse: 0.1314989483932221, rmse: 0.3626278373115088, mean: -0.0054388869963936624
std: 0.362605178176963, min: -0.7092348950441494, max: 0.6249165071485532

iteration: 22070, loss: -22.519327, current train MSE: 0.1699956341685795, lowers train MSE: 0.13606419909623213, training time: 2001.0119s
Formula: 0.999*x_2 + 0.498
model's performance on test data:
mse: 40.25559910305383, rmse: 6.344730026017958, mean: 1.557548972718368
std: 6.150888039032849, min: -6.750170944856129, max: 16.199281068812788

iteration: 0, loss: 57.097716, current train MSE: 49.95283383123423, lowers train MSE: 49.95283383123423, training time: 2.3814s
model's performance on test data:
mse: 0.13571107922034956, rmse: 0.3683898467932437, mean: 0.05519711983749965
std: 0.3642493994997216, min: -0.7738189518245022, max: 0.7160788305599632

iteration: 2000, loss: -18.739686, current train MSE: 0.40749155991407715, lowers train MSE: 0.14156765236395696, training time: 146.3633s
model's performance on test data:
mse: 0.1495621696003725, rmse: 0.38673268493931634, mean: 0.0575638426630816
std: 0.38244372139474037, min: -0.873409320407573, max: 0.8243303604896433

iteration: 4000, loss: -21.291546, current train MSE: 0.22432786151414577, lowers train MSE: 0.12114671714980492, training time: 296.4453s
model's performance on test data:
mse: 0.1423515160669974, rmse: 0.37729499873043293, mean: 0.06069689393117377
std: 0.37239934391875307, min: -0.7726707293081372, max: 0.8015137185363184

iteration: 6000, loss: -22.098964, current train MSE: 0.265895642113294, lowers train MSE: 0.1125140693890005, training time: 453.2470s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 8000, loss: -22.389580, current train MSE: 0.3270290751095261, lowers train MSE: 0.10455738164913475, training time: 618.2060s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 10000, loss: -22.586139, current train MSE: 0.18959847235448316, lowers train MSE: 0.10455738164913475, training time: 790.5057s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 12000, loss: -22.636101, current train MSE: 0.14741237415340472, lowers train MSE: 0.10455738164913475, training time: 970.0957s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 14000, loss: -22.642910, current train MSE: 0.14473077660928768, lowers train MSE: 0.10455738164913475, training time: 1158.6448s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 16000, loss: -22.644873, current train MSE: 0.14730086117279756, lowers train MSE: 0.10455738164913475, training time: 1353.6295s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 18000, loss: -22.650897, current train MSE: 0.1452928940588142, lowers train MSE: 0.10455738164913475, training time: 1557.5330s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 20000, loss: -22.659775, current train MSE: 0.14083409947491882, lowers train MSE: 0.10455738164913475, training time: 1769.1120s
model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 22000, loss: -22.665210, current train MSE: 0.1396683503955223, lowers train MSE: 0.10455738164913475, training time: 1989.4812s
2000 seconds time exceeded

model's performance on test data:
mse: 0.14430964523796194, rmse: 0.37988109355160327, mean: 0.03520427704901806
std: 0.3782652675956917, min: -0.8387441509499727, max: 0.8081474601733856

iteration: 22100, loss: -22.663343, current train MSE: 0.14154907968404848, lowers train MSE: 0.10455738164913475, training time: 2001.1198s
Formula: 0.001*x_1 + 0.997*x_2 + 0.485
model's performance on test data:
mse: 30.727300708852773, rmse: 5.543221149192298, mean: 0.3513538761343382
std: 5.532351387051096, min: -10.290741490257167, max: 10.41615290480481

iteration: 0, loss: 29.508473, current train MSE: 29.72917807706986, lowers train MSE: 29.72917807706986, training time: 0.1280s
model's performance on test data:
mse: 0.1361698194829456, rmse: 0.36901195032538664, mean: -0.0043909828299632265
std: 0.3690042753969047, min: -0.8054695145444217, max: 0.691132449517406

iteration: 2000, loss: -19.460477, current train MSE: 0.40781989149438447, lowers train MSE: 0.1244413141797395, training time: 142.3145s
model's performance on test data:
mse: 0.13099533312941708, rmse: 0.3619327743233777, mean: 0.031350657580141765
std: 0.36059044917631544, min: -0.6405660484511113, max: 0.7171770619930431

iteration: 4000, loss: -21.349484, current train MSE: 0.38465176555649183, lowers train MSE: 0.11565126154703562, training time: 292.2096s

##########
reduced lr to 0.0001
##########

Formula: 0.988*x_2 + 0.47*cos(0.045*x_2)
model's performance on test data:
mse: 40.4780449593595, rmse: 6.362235845939657, mean: 0.05967970064676322
std: 6.362274056198185, min: -14.487131182734755, max: 13.524939191626832

iteration: 0, loss: 34.838658, current train MSE: 33.770747830981556, lowers train MSE: 33.770747830981556, training time: 0.8353s
model's performance on test data:
mse: 0.14006183518093804, rmse: 0.37424836029158237, mean: 0.012928807919782221
std: 0.37404367669929656, min: -0.8887185852767789, max: 0.6912410460616623

iteration: 2000, loss: -18.902858, current train MSE: 0.40472240678686716, lowers train MSE: 0.13531236328146903, training time: 145.1260s
model's performance on test data:
mse: 0.1548468652923638, rmse: 0.3935058643684536, mean: 0.09134011556400198
std: 0.3827773511335336, min: -0.813064264831409, max: 0.8093068482306993

iteration: 4000, loss: -20.701686, current train MSE: 0.26871364960219346, lowers train MSE: 0.11516698113217153, training time: 293.6713s
Formula: 0.015*x_1 + 1.005*x_2 + 0.334*cos(0.04*x_2)
model's performance on test data:
mse: 44.34050404071948, rmse: 6.658866573278029, mean: -2.288622370181979
std: 6.253528793805114, min: -17.378309003415808, max: 10.795473076049788

iteration: 0, loss: 53.915804, current train MSE: 47.124671306644245, lowers train MSE: 47.124671306644245, training time: 0.7975s
model's performance on test data:
mse: 0.13300161173478803, rmse: 0.364693860292147, mean: 0.05492139167273017
std: 0.36055270401910877, min: -0.7114581619659255, max: 0.6395415270070215

iteration: 2000, loss: -19.182578, current train MSE: 0.32735874878439597, lowers train MSE: 0.13620866170808898, training time: 143.7375s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13300161173478803, rmse: 0.364693860292147, mean: 0.05492139167273017
std: 0.36055270401910877, min: -0.7114581619659255, max: 0.6395415270070215

iteration: 4000, loss: -20.111186, current train MSE: 0.2212039432709929, lowers train MSE: 0.13620866170808898, training time: 291.8944s

##########
reduced lr to 1e-05
##########

Formula: 1.0*x_2 + 0.453*cos(0.132*x_2)
model's performance on test data:
mse: 49.16785419532005, rmse: 7.011979335060825, mean: 0.9341420993521741
std: 6.949824658270207, min: -18.731182003896592, max: 23.196782796980585

iteration: 0, loss: 47.856350, current train MSE: 47.705822433229265, lowers train MSE: 47.705822433229265, training time: 0.8047s
model's performance on test data:
mse: 0.13428017618809338, rmse: 0.36644259603394, mean: 0.03454400047570746
std: 0.36482899873661045, min: -0.6310486988549453, max: 0.6440392530855163

iteration: 2000, loss: -20.293034, current train MSE: 0.24957919190707983, lowers train MSE: 0.12233245834752662, training time: 144.1722s
model's performance on test data:
mse: 0.13169859415627863, rmse: 0.36290300929625624, mean: 0.022649798620820755
std: 0.36221361201339003, min: -0.6343001754717079, max: 0.6246281701839678

iteration: 4000, loss: -21.754715, current train MSE: 0.23420418709101892, lowers train MSE: 0.11249023637006783, training time: 293.3238s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13169859415627863, rmse: 0.36290300929625624, mean: 0.022649798620820755
std: 0.36221361201339003, min: -0.6343001754717079, max: 0.6246281701839678

iteration: 6000, loss: -22.297353, current train MSE: 0.15814419675002478, lowers train MSE: 0.11249023637006783, training time: 451.0368s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13169859415627863, rmse: 0.36290300929625624, mean: 0.022649798620820755
std: 0.36221361201339003, min: -0.6343001754717079, max: 0.6246281701839678

iteration: 8000, loss: -22.342528, current train MSE: 0.15742516165591297, lowers train MSE: 0.11249023637006783, training time: 616.7496s
Formula: 1.0*x_2 + 0.495
model's performance on test data:
mse: 34.492040070603416, rmse: 5.872992428958462, mean: 0.3141142881440897
std: 5.864879537135629, min: -12.819296171262238, max: 13.151840131046056

iteration: 0, loss: 28.904309, current train MSE: 29.01884606614209, lowers train MSE: 29.01884606614209, training time: 1.2827s
model's performance on test data:
mse: 0.12908772615382597, rmse: 0.35928780407053335, mean: 0.01995610922906859
std: 0.3587510978050606, min: -0.5754683837872303, max: 0.5947192215731469

iteration: 2000, loss: -19.673342, current train MSE: 0.2247703328305795, lowers train MSE: 0.11796265407042131, training time: 144.3673s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12908772615382597, rmse: 0.35928780407053335, mean: 0.01995610922906859
std: 0.3587510978050606, min: -0.5754683837872303, max: 0.5947192215731469

iteration: 4000, loss: -21.339752, current train MSE: 0.1706016566533316, lowers train MSE: 0.11796265407042131, training time: 293.7489s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12908772615382597, rmse: 0.35928780407053335, mean: 0.01995610922906859
std: 0.3587510978050606, min: -0.5754683837872303, max: 0.5947192215731469

iteration: 6000, loss: -21.470810, current train MSE: 0.1674818848424262, lowers train MSE: 0.11796265407042131, training time: 449.7307s
model's performance on test data:
mse: 0.12908772615382597, rmse: 0.35928780407053335, mean: 0.01995610922906859
std: 0.3587510978050606, min: -0.5754683837872303, max: 0.5947192215731469

iteration: 8000, loss: -21.495164, current train MSE: 0.16455250958431444, lowers train MSE: 0.11796265407042131, training time: 614.2449s
Formula: 0.997*x_2 + 0.497
model's performance on test data:
mse: 34.44742276450813, rmse: 5.869192684220559, mean: 0.4367804724082707
std: 5.853210368128881, min: -10.93798708582859, max: 11.501901109442228

iteration: 0, loss: 40.840473, current train MSE: 33.64619205804101, lowers train MSE: 33.64619205804101, training time: 1.3296s
model's performance on test data:
mse: 0.14030713590921848, rmse: 0.37457594144474693, mean: 0.0985485838213649
std: 0.36139780499452623, min: -0.7046908493606292, max: 0.7463442904788709

iteration: 2000, loss: -19.284789, current train MSE: 0.2685578346337719, lowers train MSE: 0.1086214506855699, training time: 145.5655s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.14030713590921848, rmse: 0.37457594144474693, mean: 0.0985485838213649
std: 0.36139780499452623, min: -0.7046908493606292, max: 0.7463442904788709

iteration: 4000, loss: -20.836996, current train MSE: 0.18618015757979403, lowers train MSE: 0.1086214506855699, training time: 295.7247s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.14030713590921848, rmse: 0.37457594144474693, mean: 0.0985485838213649
std: 0.36139780499452623, min: -0.7046908493606292, max: 0.7463442904788709

iteration: 6000, loss: -20.996624, current train MSE: 0.16769627237908316, lowers train MSE: 0.1086214506855699, training time: 453.0131s
model's performance on test data:
mse: 0.14030713590921848, rmse: 0.37457594144474693, mean: 0.0985485838213649
std: 0.36139780499452623, min: -0.7046908493606292, max: 0.7463442904788709

iteration: 8000, loss: -21.029067, current train MSE: 0.16934753779848905, lowers train MSE: 0.1086214506855699, training time: 617.9513s
Formula: 1.011*x_2
model's performance on test data:
mse: 33.708108707039315, rmse: 5.805868471386457, mean: 0.9807576843090315
std: 5.722717714803048, min: -9.626543180759557, max: 12.091617642647448

iteration: 0, loss: 27.819335, current train MSE: 27.827410739854606, lowers train MSE: 27.827410739854606, training time: 1.3427s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 2000, loss: -20.061185, current train MSE: 0.2964300243787564, lowers train MSE: 0.12384359074936502, training time: 144.7871s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 4000, loss: -20.975109, current train MSE: 0.17031918662492945, lowers train MSE: 0.12384359074936502, training time: 293.3554s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 6000, loss: -21.049689, current train MSE: 0.16711748885388447, lowers train MSE: 0.12384359074936502, training time: 449.8613s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 8000, loss: -21.097071, current train MSE: 0.16354916162641914, lowers train MSE: 0.12384359074936502, training time: 614.5163s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 10000, loss: -21.128388, current train MSE: 0.1621661487510732, lowers train MSE: 0.12384359074936502, training time: 785.8071s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 12000, loss: -21.162136, current train MSE: 0.16175244842356765, lowers train MSE: 0.12384359074936502, training time: 964.3575s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 14000, loss: -21.191024, current train MSE: 0.1618144058204103, lowers train MSE: 0.12384359074936502, training time: 1152.9329s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 16000, loss: -21.220388, current train MSE: 0.1615463144000077, lowers train MSE: 0.12384359074936502, training time: 1349.3825s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 18000, loss: -21.247788, current train MSE: 0.16172018198169272, lowers train MSE: 0.12384359074936502, training time: 1554.8995s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 20000, loss: -21.274434, current train MSE: 0.16182026123470578, lowers train MSE: 0.12384359074936502, training time: 1768.5724s
model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 22000, loss: -21.301516, current train MSE: 0.1608295368670075, lowers train MSE: 0.12384359074936502, training time: 1988.1224s
2000 seconds time exceeded

model's performance on test data:
mse: 0.1341131407254236, rmse: 0.3662146102020284, mean: 0.035756745972871214
std: 0.36448303214437844, min: -0.6583165686988721, max: 0.7042214658007939

iteration: 22110, loss: -21.270652, current train MSE: 0.19310325506483242, lowers train MSE: 0.12384359074936502, training time: 2000.8827s
Formula: 0.998*x_2 + 0.496
model's performance on test data:
mse: 33.287576922432635, rmse: 5.769538709674512, mean: 1.2252981960307663
std: 5.638208954666409, min: -9.710714736853225, max: 13.436348915146382

iteration: 0, loss: 40.395607, current train MSE: 39.73035522761988, lowers train MSE: 39.73035522761988, training time: 0.1751s
model's performance on test data:
mse: 0.13461962739371622, rmse: 0.36690547473936147, mean: 0.04853546490175181
std: 0.3636992766910026, min: -0.7466691644261676, max: 0.6731177678656444

iteration: 2000, loss: -19.518079, current train MSE: 0.4546047135761274, lowers train MSE: 0.12478229677156889, training time: 143.1720s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 4000, loss: -21.220933, current train MSE: 0.5225334307948136, lowers train MSE: 0.11824690452148304, training time: 291.4208s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 6000, loss: -21.814133, current train MSE: 0.20622684890242504, lowers train MSE: 0.11824690452148304, training time: 448.3779s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 8000, loss: -21.877045, current train MSE: 0.17938548940435087, lowers train MSE: 0.11824690452148304, training time: 613.6537s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 10000, loss: -21.895357, current train MSE: 0.17703208509462226, lowers train MSE: 0.11824690452148304, training time: 784.9013s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 12000, loss: -21.913273, current train MSE: 0.175333192804101, lowers train MSE: 0.11824690452148304, training time: 963.8726s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 14000, loss: -21.925747, current train MSE: 0.17906463627376717, lowers train MSE: 0.11824690452148304, training time: 1152.5154s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 16000, loss: -21.941790, current train MSE: 0.17869869962322033, lowers train MSE: 0.11824690452148304, training time: 1348.3742s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 18000, loss: -21.958344, current train MSE: 0.17727773016019843, lowers train MSE: 0.11824690452148304, training time: 1551.9098s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 20000, loss: -21.973051, current train MSE: 0.17732369840532858, lowers train MSE: 0.11824690452148304, training time: 1763.4950s
model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 22000, loss: -21.985288, current train MSE: 0.17943299259947082, lowers train MSE: 0.11824690452148304, training time: 1982.6418s
2000 seconds time exceeded

model's performance on test data:
mse: 0.13796223778729833, rmse: 0.37143268271289526, mean: 0.06219871426826328
std: 0.36620618309243697, min: -0.7278475520430181, max: 0.6902774076986908

iteration: 22160, loss: -22.002388, current train MSE: 0.1634839504570168, lowers train MSE: 0.11824690452148304, training time: 2000.8712s
Formula: 0.997*x_2 + 0.496
model's performance on test data:
mse: 33.485635549310366, rmse: 5.786677418805231, mean: 0.24805567882205914
std: 5.781647403657306, min: -10.324188190280664, max: 12.171553427893231

iteration: 0, loss: 50.811414, current train MSE: 43.53439033407034, lowers train MSE: 43.53439033407034, training time: 0.1745s
model's performance on test data:
mse: 0.13515469959634288, rmse: 0.36763392062803846, mean: 0.044313505610685255
std: 0.36497168839523425, min: -0.7975817466218018, max: 0.6600359861071219

iteration: 2000, loss: -18.895976, current train MSE: 0.3520888192479745, lowers train MSE: 0.13402689526108827, training time: 143.2611s
model's performance on test data:
mse: 0.12759278189422982, rmse: 0.357201318438538, mean: -0.013364229131219941
std: 0.3569690771471313, min: -0.7266804177082129, max: 0.5669802528305947

iteration: 4000, loss: -21.153506, current train MSE: 0.3904997232685408, lowers train MSE: 0.12475578521554301, training time: 293.1235s

##########
reduced lr to 0.0001
##########

model's performance on test data:
mse: 0.12759278189422982, rmse: 0.357201318438538, mean: -0.013364229131219941
std: 0.3569690771471313, min: -0.7266804177082129, max: 0.5669802528305947

iteration: 6000, loss: -21.900550, current train MSE: 0.18069749767082427, lowers train MSE: 0.12475578521554301, training time: 450.6665s

##########
reduced lr to 1e-05
##########

model's performance on test data:
mse: 0.12759278189422982, rmse: 0.357201318438538, mean: -0.013364229131219941
std: 0.3569690771471313, min: -0.7266804177082129, max: 0.5669802528305947

iteration: 8000, loss: -21.964549, current train MSE: 0.17723940811846856, lowers train MSE: 0.12475578521554301, training time: 615.2657s
model's performance on test data:
mse: 0.12759278189422982, rmse: 0.357201318438538, mean: -0.013364229131219941
std: 0.3569690771471313, min: -0.7266804177082129, max: 0.5669802528305947

iteration: 10000, loss: -21.979895, current train MSE: 0.176941689155684, lowers train MSE: 0.12475578521554301, training time: 786.9563s
model's performance on test data:
mse: 0.09823178749261348, rmse: 0.31341950719860034, mean: 0.10571549749639039
std: 0.29506732724060475, min: -0.5192290705215328, max: 0.4948589515764432

iteration: 12000, loss: -22.023124, current train MSE: 0.14808174423794065, lowers train MSE: 0.11949622088144254, training time: 966.5957s
model's performance on test data:
mse: 0.08683884465998588, rmse: 0.29468431356281233, mean: 0.09548937566942978
std: 0.27879812890501504, min: -0.5366913813661398, max: 0.5119806960752964

iteration: 14000, loss: -22.052239, current train MSE: 0.13339721250146813, lowers train MSE: 0.11504494555805185, training time: 1156.1948s
model's performance on test data:
mse: 0.06441574930763883, rmse: 0.2538025793951646, mean: 0.07777818018447288
std: 0.24160327242728222, min: -0.49212038669678826, max: 0.4979191276080408

iteration: 16000, loss: -22.088617, current train MSE: 0.10787510358781334, lowers train MSE: 0.09947717053064473, training time: 1352.7339s
model's performance on test data:
mse: 0.04950281121819601, rmse: 0.22249227226624302, mean: 0.0327468644755611
std: 0.22008020730869665, min: -0.48913379710080385, max: 0.4671100123841774

iteration: 18000, loss: -22.104055, current train MSE: 0.10337610187573971, lowers train MSE: 0.08892819485552556, training time: 1556.9162s
model's performance on test data:
mse: 0.05074192328755704, rmse: 0.22525967967560692, mean: 0.047386656145037995
std: 0.22023005759579894, min: -0.4788794803759995, max: 0.47825797815602034

iteration: 20000, loss: -22.119260, current train MSE: 0.10352606640128426, lowers train MSE: 0.08779375941932983, training time: 1768.6033s
model's performance on test data:
mse: 0.05090861754432573, rmse: 0.22562938094212315, mean: 0.04464851321712019
std: 0.22117870568455922, min: -0.49785159488738806, max: 0.4878771555564274

iteration: 22000, loss: -22.136313, current train MSE: 0.10137383179656162, lowers train MSE: 0.08586209181887458, training time: 1988.0210s
2000 seconds time exceeded

model's performance on test data:
mse: 0.05090861754432573, rmse: 0.22562938094212315, mean: 0.04464851321712019
std: 0.22117870568455922, min: -0.49785159488738806, max: 0.4878771555564274

iteration: 22110, loss: -22.129372, current train MSE: 0.10925250316809287, lowers train MSE: 0.08586209181887458, training time: 2000.5102s
Formula: 0.997*x_2 + 0.529*cos(0.382*x_1*sin(x_1))
